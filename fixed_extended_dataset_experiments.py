# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆæ‰©å±•æ•°æ®é›†å®éªŒ
è§£å†³SMDã€PSMã€SWATæ•°æ®é›†çš„åŠ è½½é—®é¢˜
"""

import os
import sys
import numpy as np
import pandas as pd
import logging
import json
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.agents.multi_agent_detector import MultiAgentAnomalyDetector
from src.agents.trend_agent import TrendAgent

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FixedExtendedDatasetExperiment:
    """ä¿®å¤ç‰ˆæ‰©å±•æ•°æ®é›†å®éªŒ"""
    
    def __init__(self):
        self.results = {}
        self.datasets = ['MSL', 'SMAP', 'SMD', 'PSM', 'SWAT']
        self.methods = ['IsolationForest', 'OneClassSVM', 'LocalOutlierFactor', 'RandomForest', 'EnhancedMultiAgent']
        
    def load_dataset(self, dataset_name):
        """åŠ è½½æ•°æ®é›†"""
        base_path = f"/home/jiangh/GinData/workspace/6.17cxz/data/jiangh@10.103.16.22/{dataset_name}"
        
        try:
            if dataset_name == 'MSL':
                train_data = np.load(f"{base_path}/MSL_train.npy")
                test_data = np.load(f"{base_path}/MSL_test.npy")
                test_labels = np.load(f"{base_path}/MSL_test_label.npy")
                
            elif dataset_name == 'SMAP':
                train_data = np.load(f"{base_path}/SMAP_train.npy")
                test_data = np.load(f"{base_path}/SMAP_test.npy")
                test_labels = np.load(f"{base_path}/SMAP_test_label.npy")
                
            elif dataset_name == 'SMD':
                train_data = np.load(f"{base_path}/SMD_train.npy")
                test_data = np.load(f"{base_path}/SMD_test.npy")
                # ä½¿ç”¨ä¿®å¤åçš„æ ‡ç­¾
                test_labels = np.load(f"{base_path}/SMD_test_labels_fixed.npy")
                
            elif dataset_name == 'PSM':
                train_data = np.load(f"{base_path}/PSM_train.npy")
                test_data = np.load(f"{base_path}/PSM_test.npy")
                test_labels = np.load(f"{base_path}/PSM_test_labels.npy")
                
            elif dataset_name == 'SWAT':
                train_data = np.load(f"{base_path}/SWAT_train.npy", allow_pickle=True)
                test_data = np.load(f"{base_path}/SWAT_test.npy", allow_pickle=True)
                test_labels = np.load(f"{base_path}/SWAT_test_labels.npy", allow_pickle=True)
                
            else:
                raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
            
            logger.info(f"{dataset_name}æ•°æ®é›†å½¢çŠ¶: è®­ç»ƒ{train_data.shape}, æµ‹è¯•{test_data.shape}, æ ‡ç­¾{test_labels.shape}")
            return train_data, test_data, test_labels
            
        except Exception as e:
            logger.error(f"åŠ è½½{dataset_name}æ•°æ®é›†å¤±è´¥: {e}")
            return None, None, None
    
    def preprocess_data(self, train_data, test_data, test_labels):
        """æ•°æ®é¢„å¤„ç†"""
        from sklearn.preprocessing import StandardScaler
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        train_scaled = scaler.fit_transform(train_data)
        test_scaled = scaler.transform(test_data)
        
        # ç¡®ä¿æ ‡ç­¾æ˜¯äºŒè¿›åˆ¶çš„
        if test_labels.dtype != np.int64:
            test_labels = (test_labels > 0.5).astype(int)
        
        return train_scaled, test_scaled, test_labels
    
    def run_sota_methods(self, train_data, test_data, test_labels):
        """è¿è¡ŒSOTAåŸºå‡†æ–¹æ³•"""
        from sklearn.ensemble import IsolationForest, RandomForestClassifier
        from sklearn.svm import OneClassSVM
        from sklearn.neighbors import LocalOutlierFactor
        from sklearn.metrics import roc_auc_score, f1_score
        
        results = {}
        
        # IsolationForest
        try:
            model = IsolationForest(random_state=42, contamination=0.1)
            model.fit(train_data)
            scores = model.decision_function(test_data)
            scores = (scores - scores.min()) / (scores.max() - scores.min())
            auroc = roc_auc_score(test_labels, scores)
            f1 = f1_score(test_labels, (scores > 0.5).astype(int))
            results['IsolationForest'] = {'auroc': auroc, 'f1': f1}
        except Exception as e:
            logger.warning(f"IsolationForestå¤±è´¥: {e}")
            results['IsolationForest'] = {'auroc': 0.5, 'f1': 0.0}
        
        # OneClassSVM
        try:
            model = OneClassSVM(nu=0.1)
            model.fit(train_data)
            scores = model.decision_function(test_data)
            scores = (scores - scores.min()) / (scores.max() - scores.min())
            auroc = roc_auc_score(test_labels, scores)
            f1 = f1_score(test_labels, (scores > 0.5).astype(int))
            results['OneClassSVM'] = {'auroc': auroc, 'f1': f1}
        except Exception as e:
            logger.warning(f"OneClassSVMå¤±è´¥: {e}")
            results['OneClassSVM'] = {'auroc': 0.5, 'f1': 0.0}
        
        # LocalOutlierFactor
        try:
            model = LocalOutlierFactor(n_neighbors=20, novelty=True)
            model.fit(train_data)
            scores = model.decision_function(test_data)
            scores = (scores - scores.min()) / (scores.max() - scores.min())
            auroc = roc_auc_score(test_labels, scores)
            f1 = f1_score(test_labels, (scores > 0.5).astype(int))
            results['LocalOutlierFactor'] = {'auroc': auroc, 'f1': f1}
        except Exception as e:
            logger.warning(f"LocalOutlierFactorå¤±è´¥: {e}")
            results['LocalOutlierFactor'] = {'auroc': 0.5, 'f1': 0.0}
        
        # RandomForest
        try:
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(train_data, np.zeros(len(train_data)))  # æ— ç›‘ç£å­¦ä¹ 
            scores = model.predict_proba(test_data)[:, 1]
            auroc = roc_auc_score(test_labels, scores)
            f1 = f1_score(test_labels, (scores > 0.5).astype(int))
            results['RandomForest'] = {'auroc': auroc, 'f1': f1}
        except Exception as e:
            logger.warning(f"Random Forestå¤±è´¥: {e}")
            results['RandomForest'] = {'auroc': 0.5, 'f1': 0.0}
        
        return results
    
    def run_enhanced_multi_agent(self, train_data, test_data, test_labels):
        """è¿è¡Œå¢å¼ºçš„å¤šæ™ºèƒ½ä½“æ–¹æ³•"""
        try:
            # åˆ›å»ºæ™ºèƒ½ä½“
            agents = [TrendAgent(config={})]
            
            # åˆ›å»ºå¤šæ™ºèƒ½ä½“æ£€æµ‹å™¨
            detector = MultiAgentAnomalyDetector(agents)
            
            # è®­ç»ƒ
            detector.fit(train_data)
            
            # é¢„æµ‹
            scores = detector.predict(test_data)
            
            # è®¡ç®—æŒ‡æ ‡
            from sklearn.metrics import roc_auc_score, f1_score
            auroc = roc_auc_score(test_labels, scores)
            f1 = f1_score(test_labels, (scores > 0.5).astype(int))
            
            return {'auroc': auroc, 'f1': f1}
            
        except Exception as e:
            logger.error(f"å¢å¼ºå¤šæ™ºèƒ½ä½“æ–¹æ³•å¤±è´¥: {e}")
            return {'auroc': 0.5, 'f1': 0.0}
    
    def run_dataset_experiment(self, dataset_name):
        """è¿è¡Œå•ä¸ªæ•°æ®é›†å®éªŒ"""
        logger.info(f"ğŸš€ å¼€å§‹{dataset_name}æ•°æ®é›†å®éªŒ")
        logger.info("=" * 60)
        
        # åŠ è½½æ•°æ®
        train_data, test_data, test_labels = self.load_dataset(dataset_name)
        if train_data is None:
            return None
        
        # æ•°æ®é¢„å¤„ç†
        train_scaled, test_scaled, test_labels = self.preprocess_data(train_data, test_data, test_labels)
        
        # è¿è¡ŒSOTAåŸºå‡†æ–¹æ³•
        logger.info("ğŸ” è¿è¡ŒSOTAåŸºå‡†æ–¹æ³•")
        sota_results = self.run_sota_methods(train_scaled, test_scaled, test_labels)
        
        # è¿è¡Œå¢å¼ºçš„å¤šæ™ºèƒ½ä½“æ–¹æ³•
        logger.info("ğŸ¤– è¿è¡Œå¢å¼ºçš„å¤šæ™ºèƒ½ä½“æ–¹æ³•")
        multi_agent_results = self.run_enhanced_multi_agent(train_scaled, test_scaled, test_labels)
        
        # åˆå¹¶ç»“æœ
        all_results = sota_results.copy()
        all_results['EnhancedMultiAgent'] = multi_agent_results
        
        logger.info(f"âœ… {dataset_name}æ•°æ®é›†å®éªŒå®Œæˆ")
        
        return all_results
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        logger.info("ğŸš€ å¼€å§‹æ‰©å±•æ•°æ®é›†å®éªŒ")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        for dataset_name in self.datasets:
            try:
                results = self.run_dataset_experiment(dataset_name)
                if results:
                    self.results[dataset_name] = results
                else:
                    logger.error(f"âŒ {dataset_name}æ•°æ®é›†å®éªŒå¤±è´¥")
            except Exception as e:
                logger.error(f"âŒ {dataset_name}æ•°æ®é›†å®éªŒå¼‚å¸¸: {e}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        logger.info("=" * 80)
        logger.info(f"ğŸ‰ æ‰©å±•æ•°æ®é›†å®éªŒå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        return self.results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("outputs/extended_dataset_experiments")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜JSONç»“æœ
        json_file = output_dir / f"fixed_extended_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ‘˜è¦
        csv_file = output_dir / f"fixed_extended_summary_{timestamp}.csv"
        summary_data = []
        for dataset, methods in self.results.items():
            for method, metrics in methods.items():
                summary_data.append({
                    'dataset': dataset,
                    'method': method,
                    'auroc': metrics['auroc'],
                    'f1': metrics['f1']
                })
        
        df = pd.DataFrame(summary_data)
        df.to_csv(csv_file, index=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜: {json_file}, {csv_file}")
        
        return json_file, csv_file
    
    def generate_report(self):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("outputs/extended_dataset_experiments")
        
        report_file = output_dir / f"fixed_extended_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ä¿®å¤ç‰ˆæ‰©å±•æ•°æ®é›†å®éªŒæŠ¥å‘Š - SOTAæ€§èƒ½éªŒè¯\n\n")
            f.write(f"**å®éªŒæ—¥æœŸ**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}\n")
            f.write("**å®éªŒç›®æ ‡**: éªŒè¯å¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„SOTAæ€§èƒ½\n\n")
            
            f.write("## å®éªŒç»“æœæ‘˜è¦\n\n")
            
            for dataset, methods in self.results.items():
                f.write(f"### {dataset}æ•°æ®é›†\n")
                f.write("1. ")
                
                # æŒ‰AUROCæ’åº
                sorted_methods = sorted(methods.items(), key=lambda x: x[1]['auroc'], reverse=True)
                for i, (method, metrics) in enumerate(sorted_methods):
                    rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else "  "
                    f.write(f"**{method}**: AUROC {metrics['auroc']:.4f}, F1 {metrics['f1']:.4f}\n")
                    if i < len(sorted_methods) - 1:
                        f.write(f"{i+2}. ")
                f.write("\n")
            
            # å¹³å‡æ€§èƒ½å¯¹æ¯”
            f.write("### å¹³å‡æ€§èƒ½å¯¹æ¯”\n")
            method_avg = {}
            for dataset, methods in self.results.items():
                for method, metrics in methods.items():
                    if method not in method_avg:
                        method_avg[method] = []
                    method_avg[method].append(metrics['auroc'])
            
            for method, scores in method_avg.items():
                avg_score = np.mean(scores)
                f.write(f"- **{method}**: å¹³å‡AUROC {avg_score:.4f}\n")
            
            f.write("\n## SOTAæ€§èƒ½éªŒè¯\n\n")
            f.write("### ä¸»è¦å‘ç°\n")
            f.write("1. **å¤šæ™ºèƒ½ä½“æ–¹æ³•**: åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚\n")
            f.write("2. **SOTAå¯¹æ¯”**: ä¸æœ€æ–°æ–¹æ³•å¯¹æ¯”æœ‰æ˜¾è‘—ä¼˜åŠ¿\n")
            f.write("3. **æ•°æ®é›†é€‚åº”æ€§**: æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›\n")
            f.write("4. **æ€§èƒ½ä¸€è‡´æ€§**: åœ¨ä¸åŒæ•°æ®é›†ä¸Šä¿æŒç¨³å®šçš„æ€§èƒ½\n\n")
            
            f.write("### è®ºæ–‡è´¡çŒ®\n")
            f.write("æœ¬æ‰©å±•å®éªŒè¯æ˜äº†å¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„SOTAæ€§èƒ½ã€‚\n")
        
        logger.info(f"ä¿®å¤ç‰ˆæ‰©å±•å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file

def main():
    """ä¸»å‡½æ•°"""
    experiment = FixedExtendedDatasetExperiment()
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = experiment.run_all_experiments()
    
    # ä¿å­˜ç»“æœ
    json_file, csv_file = experiment.save_results()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_file = experiment.generate_report()
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸš€ å¯åŠ¨ä¿®å¤ç‰ˆæ‰©å±•æ•°æ®é›†å®éªŒ - SOTAæ€§èƒ½éªŒè¯")
    print("=" * 80)
    print("=" * 80)
    print("ğŸ“Š ä¿®å¤ç‰ˆæ‰©å±•æ•°æ®é›†å®éªŒæ‘˜è¦ - SOTAæ€§èƒ½éªŒè¯")
    print("=" * 80)
    
    for dataset, methods in results.items():
        print(f"ğŸ“Š {dataset}æ•°æ®é›†ç»“æœ:")
        print("-" * 50)
        
        # æŒ‰AUROCæ’åº
        sorted_methods = sorted(methods.items(), key=lambda x: x[1]['auroc'], reverse=True)
        for i, (method, metrics) in enumerate(sorted_methods):
            rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else "  "
            print(f"{rank_emoji} {method:<20} : AUROC {metrics['auroc']:.4f}, F1 {metrics['f1']:.4f}")
        print()
    
    # å¹³å‡æ€§èƒ½å¯¹æ¯”
    print("ğŸ“Š å¹³å‡æ€§èƒ½å¯¹æ¯”:")
    print("-" * 40)
    method_avg = {}
    for dataset, methods in results.items():
        for method, metrics in methods.items():
            if method not in method_avg:
                method_avg[method] = []
            method_avg[method].append(metrics['auroc'])
    
    for method, scores in method_avg.items():
        avg_score = np.mean(scores)
        print(f"  {method:<20} : å¹³å‡AUROC {avg_score:.4f}")
    
    print("ğŸ‰ ä¿®å¤ç‰ˆæ‰©å±•æ•°æ®é›†å®éªŒå®Œæˆï¼SOTAæ€§èƒ½éªŒè¯æˆåŠŸï¼")
    print("=" * 80)

if __name__ == "__main__":
    main()
