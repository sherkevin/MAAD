# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆLLMé©±åŠ¨é€šä¿¡ç³»ç»Ÿ
é›†æˆé˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°ï¼Œæä¾›æ›´æ™ºèƒ½çš„å¤šæ™ºèƒ½ä½“åä½œ
"""

import logging
import time
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
import json

from src.llm.aliyun_qwen_interface import AliyunQwenInterface
from src.communication.t2mac_protocol import T2MACProtocol

class EnhancedLLMCommunication:
    """å¢å¼ºç‰ˆLLMé©±åŠ¨é€šä¿¡ç³»ç»Ÿ"""
    
    def __init__(self, api_key: str, model: str = "qwen-turbo"):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆé€šä¿¡ç³»ç»Ÿ
        
        Args:
            api_key: é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥
            model: æ¨¡å‹åç§°
        """
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–LLMæ¥å£
        self.llm_interface = AliyunQwenInterface(api_key, model)
        
        # åˆå§‹åŒ–T2MACåè®®
        self.t2mac = T2MACProtocol(config={})
        
        # é€šä¿¡çŠ¶æ€
        self.communication_history = []
        self.agent_performance = {}
        self.fusion_strategies = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_communications = 0
        self.successful_communications = 0
        self.llm_calls = 0
        
    def intelligent_agent_coordination(self, agent_results: List[Dict], context: Dict) -> Dict:
        """
        æ™ºèƒ½æ™ºèƒ½ä½“åè°ƒ
        
        Args:
            agent_results: æ™ºèƒ½ä½“æ£€æµ‹ç»“æœ
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            åè°ƒç»“æœ
        """
        self.logger.info("ğŸ¤– å¼€å§‹æ™ºèƒ½æ™ºèƒ½ä½“åè°ƒ")
        
        # 1. åˆ†æå„æ™ºèƒ½ä½“ç»“æœ
        analysis_result = self._analyze_agent_results(agent_results, context)
        
        # 2. ç”Ÿæˆèåˆç­–ç•¥
        fusion_strategy = self._generate_fusion_strategy(agent_results)
        
        # 3. åè°ƒæ™ºèƒ½ä½“åä½œ
        coordination_result = self._coordinate_agents(agent_results, analysis_result)
        
        # 4. ç”Ÿæˆæœ€ç»ˆå†³ç­–
        final_decision = self._generate_final_decision(
            agent_results, analysis_result, fusion_strategy, coordination_result
        )
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.total_communications += 1
        if final_decision.get('success', False):
            self.successful_communications += 1
        
        # è®°å½•é€šä¿¡å†å²
        self.communication_history.append({
            'timestamp': time.time(),
            'agent_results': agent_results,
            'coordination_result': coordination_result,
            'final_decision': final_decision
        })
        
        return final_decision
    
    def _analyze_agent_results(self, agent_results: List[Dict], context: Dict) -> Dict:
        """åˆ†ææ™ºèƒ½ä½“ç»“æœ"""
        self.logger.info("ğŸ“Š åˆ†ææ™ºèƒ½ä½“æ£€æµ‹ç»“æœ")
        
        # æå–åˆ†æ•°å’Œç½®ä¿¡åº¦
        scores = [result.get('score', 0) for result in agent_results]
        confidences = [result.get('confidence', 0) for result in agent_results]
        agent_names = [result.get('agent', f'Agent_{i}') for i, result in enumerate(agent_results)]
        
        # ä½¿ç”¨LLMåˆ†æå¼‚å¸¸æ¨¡å¼
        analysis_result = self.llm_interface.analyze_anomaly_patterns(agent_results, context)
        self.llm_calls += 1
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        max_score = np.max(scores)
        min_score = np.min(scores)
        
        # ä¸€è‡´æ€§åˆ†æ
        consistency = 1.0 - (std_score / max(mean_score, 0.001))
        
        return {
            'scores': scores,
            'confidences': confidences,
            'agent_names': agent_names,
            'mean_score': mean_score,
            'std_score': std_score,
            'max_score': max_score,
            'min_score': min_score,
            'consistency': consistency,
            'llm_analysis': analysis_result.get('analysis', ''),
            'timestamp': time.time()
        }
    
    def _generate_fusion_strategy(self, agent_results: List[Dict]) -> Dict:
        """ç”Ÿæˆèåˆç­–ç•¥"""
        self.logger.info("ğŸ”— ç”Ÿæˆæ™ºèƒ½èåˆç­–ç•¥")
        
        # æå–åˆ†æ•°å’Œæƒé‡
        scores = [result.get('score', 0) for result in agent_results]
        weights = [result.get('weight', 1.0) for result in agent_results]
        
        # ä½¿ç”¨LLMç”Ÿæˆèåˆç­–ç•¥
        strategy_result = self.llm_interface.generate_fusion_strategy(scores, weights)
        self.llm_calls += 1
        
        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        adaptive_weights = self._calculate_adaptive_weights(agent_results)
        
        # è®¡ç®—èåˆåˆ†æ•°
        fused_score = np.average(scores, weights=adaptive_weights)
        
        return {
            'original_weights': weights,
            'adaptive_weights': adaptive_weights,
            'fused_score': fused_score,
            'strategy_advice': strategy_result.get('strategy', ''),
            'confidence': strategy_result.get('confidence', 0.5),
            'timestamp': time.time()
        }
    
    def _calculate_adaptive_weights(self, agent_results: List[Dict]) -> List[float]:
        """è®¡ç®—è‡ªé€‚åº”æƒé‡"""
        weights = []
        
        for result in agent_results:
            # åŸºäºç½®ä¿¡åº¦å’Œå†å²æ€§èƒ½è°ƒæ•´æƒé‡
            confidence = result.get('confidence', 0.5)
            agent_name = result.get('agent', 'Unknown')
            
            # è·å–å†å²æ€§èƒ½
            historical_performance = self.agent_performance.get(agent_name, 0.5)
            
            # è®¡ç®—è‡ªé€‚åº”æƒé‡
            adaptive_weight = confidence * historical_performance
            weights.append(adaptive_weight)
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights)
        if total_weight > 0:
            weights = [w / total_weight for w in weights]
        else:
            weights = [1.0 / len(weights)] * len(weights)
        
        return weights
    
    def _coordinate_agents(self, agent_results: List[Dict], analysis_result: Dict) -> Dict:
        """åè°ƒæ™ºèƒ½ä½“åä½œ"""
        self.logger.info("ğŸ¤ åè°ƒæ™ºèƒ½ä½“åä½œ")
        
        # æ„å»ºæ™ºèƒ½ä½“æ¶ˆæ¯
        agent_messages = []
        for i, result in enumerate(agent_results):
            agent_name = result.get('agent', f'Agent_{i}')
            score = result.get('score', 0)
            confidence = result.get('confidence', 0)
            
            message = {
                'agent': agent_name,
                'content': f'æ£€æµ‹åˆ†æ•°: {score:.4f}, ç½®ä¿¡åº¦: {confidence:.4f}'
            }
            agent_messages.append(message)
        
        # ä½¿ç”¨LLMè¿›è¡Œå¤šæ™ºèƒ½ä½“é€šä¿¡
        context = f"ä¸€è‡´æ€§: {analysis_result.get('consistency', 0):.4f}, å¹³å‡åˆ†æ•°: {analysis_result.get('mean_score', 0):.4f}"
        comm_result = self.llm_interface.multi_agent_communication(agent_messages, context)
        self.llm_calls += 1
        
        return {
            'communication_success': comm_result.get('success', False),
            'coordination_advice': comm_result.get('response', ''),
            'agent_messages': agent_messages,
            'timestamp': time.time()
        }
    
    def _generate_final_decision(self, agent_results: List[Dict], analysis_result: Dict, 
                                fusion_strategy: Dict, coordination_result: Dict) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆå†³ç­–"""
        self.logger.info("ğŸ¯ ç”Ÿæˆæœ€ç»ˆå¼‚å¸¸æ£€æµ‹å†³ç­–")
        
        # è®¡ç®—æœ€ç»ˆå¼‚å¸¸åˆ†æ•°
        fused_score = fusion_strategy.get('fused_score', 0)
        consistency = analysis_result.get('consistency', 0)
        confidence = fusion_strategy.get('confidence', 0.5)
        
        # åŸºäºä¸€è‡´æ€§å’Œç½®ä¿¡åº¦è°ƒæ•´æœ€ç»ˆåˆ†æ•°
        adjusted_score = fused_score * (0.7 + 0.3 * consistency) * (0.5 + 0.5 * confidence)
        
        # ç¡®å®šå¼‚å¸¸é˜ˆå€¼
        threshold = self._calculate_dynamic_threshold(agent_results, analysis_result)
        
        # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
        is_anomaly = adjusted_score > threshold
        
        # è®¡ç®—é£é™©ç­‰çº§
        risk_level = self._calculate_risk_level(adjusted_score, threshold)
        
        # ç”Ÿæˆè§£é‡Š
        explanation = self._generate_explanation(
            agent_results, analysis_result, fusion_strategy, 
            coordination_result, adjusted_score, threshold, is_anomaly
        )
        
        return {
            'success': True,
            'anomaly_score': adjusted_score,
            'threshold': threshold,
            'is_anomaly': is_anomaly,
            'risk_level': risk_level,
            'confidence': confidence,
            'consistency': consistency,
            'explanation': explanation,
            'fusion_strategy': fusion_strategy,
            'coordination_result': coordination_result,
            'timestamp': time.time()
        }
    
    def _calculate_dynamic_threshold(self, agent_results: List[Dict], analysis_result: Dict) -> float:
        """è®¡ç®—åŠ¨æ€å¼‚å¸¸é˜ˆå€¼"""
        # åŸºäºå†å²æ•°æ®å’Œä¸€è‡´æ€§è°ƒæ•´é˜ˆå€¼
        base_threshold = 0.5
        consistency = analysis_result.get('consistency', 0.5)
        std_score = analysis_result.get('std_score', 0.1)
        
        # ä¸€è‡´æ€§è¶Šé«˜ï¼Œé˜ˆå€¼è¶Šä¸¥æ ¼
        consistency_factor = 1.0 - 0.2 * consistency
        
        # æ ‡å‡†å·®è¶Šå¤§ï¼Œé˜ˆå€¼è¶Šå®½æ¾
        std_factor = 1.0 + 0.1 * std_score
        
        dynamic_threshold = base_threshold * consistency_factor * std_factor
        
        return max(0.1, min(0.9, dynamic_threshold))
    
    def _calculate_risk_level(self, score: float, threshold: float) -> str:
        """è®¡ç®—é£é™©ç­‰çº§"""
        ratio = score / max(threshold, 0.001)
        
        if ratio < 0.5:
            return "ä½é£é™©"
        elif ratio < 1.0:
            return "ä¸­é£é™©"
        elif ratio < 1.5:
            return "é«˜é£é™©"
        else:
            return "æé«˜é£é™©"
    
    def _generate_explanation(self, agent_results: List[Dict], analysis_result: Dict,
                            fusion_strategy: Dict, coordination_result: Dict,
                            final_score: float, threshold: float, is_anomaly: bool) -> str:
        """ç”Ÿæˆå†³ç­–è§£é‡Š"""
        explanation_parts = []
        
        # åŸºæœ¬å†³ç­–ä¿¡æ¯
        explanation_parts.append(f"æœ€ç»ˆå¼‚å¸¸åˆ†æ•°: {final_score:.4f} (é˜ˆå€¼: {threshold:.4f})")
        explanation_parts.append(f"æ£€æµ‹ç»“æœ: {'å¼‚å¸¸' if is_anomaly else 'æ­£å¸¸'}")
        
        # æ™ºèƒ½ä½“è´¡çŒ®åˆ†æ
        explanation_parts.append("\næ™ºèƒ½ä½“è´¡çŒ®åˆ†æ:")
        for i, result in enumerate(agent_results):
            agent_name = result.get('agent', f'Agent_{i}')
            score = result.get('score', 0)
            weight = fusion_strategy.get('adaptive_weights', [1.0] * len(agent_results))[i]
            explanation_parts.append(f"  {agent_name}: åˆ†æ•°={score:.4f}, æƒé‡={weight:.4f}")
        
        # ä¸€è‡´æ€§åˆ†æ
        consistency = analysis_result.get('consistency', 0)
        explanation_parts.append(f"\nä¸€è‡´æ€§åˆ†æ: {consistency:.4f}")
        
        # LLMåˆ†æç»“æœ
        llm_analysis = analysis_result.get('llm_analysis', '')
        if llm_analysis:
            explanation_parts.append(f"\næ™ºèƒ½åˆ†æ: {llm_analysis[:200]}...")
        
        return "\n".join(explanation_parts)
    
    def update_agent_performance(self, agent_name: str, performance_score: float):
        """æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½"""
        if agent_name not in self.agent_performance:
            self.agent_performance[agent_name] = performance_score
        else:
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            alpha = 0.1
            self.agent_performance[agent_name] = (
                alpha * performance_score + 
                (1 - alpha) * self.agent_performance[agent_name]
            )
    
    def get_communication_statistics(self) -> Dict:
        """è·å–é€šä¿¡ç»Ÿè®¡ä¿¡æ¯"""
        llm_stats = self.llm_interface.get_statistics()
        
        return {
            'total_communications': self.total_communications,
            'successful_communications': self.successful_communications,
            'success_rate': self.successful_communications / max(1, self.total_communications),
            'llm_calls': self.llm_calls,
            'llm_statistics': llm_stats,
            'agent_performance': self.agent_performance,
            'communication_history_length': len(self.communication_history)
        }
    
    def save_communication_log(self, filepath: str):
        """ä¿å­˜é€šä¿¡æ—¥å¿—"""
        log_data = {
            'statistics': self.get_communication_statistics(),
            'communication_history': self.communication_history[-100:],  # ä¿å­˜æœ€è¿‘100æ¡
            'agent_performance': self.agent_performance,
            'timestamp': time.time()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)

# æµ‹è¯•å‡½æ•°
def test_enhanced_communication():
    """æµ‹è¯•å¢å¼ºç‰ˆé€šä¿¡ç³»ç»Ÿ"""
    # æ³¨æ„ï¼šéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
    api_key = "sk-dc7f3086d0564eb6ac282c7d66faea12"
    
    # åˆ›å»ºå¢å¼ºç‰ˆé€šä¿¡ç³»ç»Ÿ
    comm_system = EnhancedLLMCommunication(api_key)
    
    # æ¨¡æ‹Ÿæ™ºèƒ½ä½“ç»“æœ
    agent_results = [
        {'agent': 'TrendAgent', 'score': 0.8, 'confidence': 0.9, 'weight': 1.0},
        {'agent': 'VarianceAgent', 'score': 0.3, 'confidence': 0.7, 'weight': 1.0},
        {'agent': 'ResidualAgent', 'score': 0.7, 'confidence': 0.8, 'weight': 1.0},
        {'agent': 'StatisticalAgent', 'score': 0.6, 'confidence': 0.6, 'weight': 1.0}
    ]
    
    context = {
        'dataset': 'MSL',
        'features': 55,
        'samples': 73729,
        'window_size': 100
    }
    
    # æµ‹è¯•æ™ºèƒ½åè°ƒ
    print("æµ‹è¯•å¢å¼ºç‰ˆå¤šæ™ºèƒ½ä½“é€šä¿¡ç³»ç»Ÿ...")
    result = comm_system.intelligent_agent_coordination(agent_results, context)
    
    print(f"åè°ƒç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = comm_system.get_communication_statistics()
    print(f"\nç»Ÿè®¡ä¿¡æ¯: {json.dumps(stats, indent=2, ensure_ascii=False)}")

if __name__ == "__main__":
    test_enhanced_communication()
