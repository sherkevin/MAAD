#!/usr/bin/env python3
"""
å¿«é€Ÿä¿®å¤æ¶ˆèç ”ç©¶å®éªŒ - ä¸ºç»„ä¼šæ±‡æŠ¥å‡†å¤‡æ•°æ®
"""

import os
import sys
import time
import torch
import numpy as np
import pandas as pd
from datetime import datetime
import json
import logging
from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QuickAblationStudy:
    """å¿«é€Ÿæ¶ˆèç ”ç©¶ - ä¸ºç»„ä¼šæ±‡æŠ¥å‡†å¤‡æ•°æ®"""
    
    def __init__(self, data_root="/home/jiangh/GinData/workspace/6.17cxz/data/jiangh@10.103.16.22", 
                 output_dir="outputs/quick_ablation_results"):
        self.data_root = data_root
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.results = {}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_msl_dataset(self):
        """åŠ è½½MSLæ•°æ®é›†"""
        logger.info("ğŸ“Š åŠ è½½MSLæ•°æ®é›†")
        
        try:
            train_data = np.load(os.path.join(self.data_root, "MSL", "MSL_train.npy"))
            test_data = np.load(os.path.join(self.data_root, "MSL", "MSL_test.npy"))
            test_labels = np.load(os.path.join(self.data_root, "MSL", "MSL_test_label.npy"))
            
            logger.info(f"MSLæ•°æ®é›†å½¢çŠ¶: è®­ç»ƒ{train_data.shape}, æµ‹è¯•{test_data.shape}, æ ‡ç­¾{test_labels.shape}")
            
            return {
                'name': 'MSL',
                'train_data': train_data,
                'test_data': test_data,
                'test_labels': test_labels,
                'num_features': train_data.shape[1],
                'train_samples': train_data.shape[0],
                'test_samples': test_data.shape[0],
                'anomaly_ratio': np.mean(test_labels)
            }
        except Exception as e:
            logger.error(f"åŠ è½½MSLæ•°æ®é›†å¤±è´¥: {e}")
            return None
    
    def preprocess_data(self, dataset):
        """æ•°æ®é¢„å¤„ç†"""
        logger.info(f"ğŸ”„ é¢„å¤„ç†{dataset['name']}æ•°æ®é›†")
        
        scaler = StandardScaler()
        train_data_scaled = scaler.fit_transform(dataset['train_data'])
        test_data_scaled = scaler.transform(dataset['test_data'])
        
        # å¤„ç†NaNå€¼
        train_data_scaled = np.nan_to_num(train_data_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        test_data_scaled = np.nan_to_num(test_data_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        
        return {
            'train_data': train_data_scaled,
            'test_data': test_data_scaled,
            'test_labels': dataset['test_labels'],
            'scaler': scaler
        }
    
    def create_agent(self, agent_type, window_size=20):
        """åˆ›å»ºå•ä¸ªæ™ºèƒ½ä½“"""
        class SingleAgent:
            def __init__(self, agent_type, window_size):
                self.agent_type = agent_type
                self.window_size = window_size
            
            def analyze(self, data):
                if self.agent_type == 'trend_analysis':
                    # è¶‹åŠ¿åˆ†æ
                    trend_scores = []
                    for i in range(len(data)):
                        start_idx = max(0, i - self.window_size)
                        window_data = data[start_idx:i+1]
                        if len(window_data) > 1:
                            mean_val = torch.mean(window_data, dim=0)
                            current_val = data[i]
                            trend = torch.mean(torch.abs(current_val - mean_val))
                            trend_scores.append(trend.item())
                        else:
                            trend_scores.append(0.0)
                    return np.array(trend_scores)
                
                elif self.agent_type == 'variance_analysis':
                    # æ–¹å·®åˆ†æ
                    variance_scores = []
                    for i in range(len(data)):
                        if i >= self.window_size:
                            recent_data = data[i-self.window_size:i]
                            current_var = torch.var(data[i])
                            recent_var = torch.var(recent_data)
                            variance_ratio = current_var / (recent_var + 1e-8)
                            variance_scores.append(torch.abs(torch.log(variance_ratio + 1e-8)).item())
                        else:
                            variance_scores.append(0.0)
                    return np.array(variance_scores)
                
                elif self.agent_type == 'residual_analysis':
                    # æ®‹å·®åˆ†æ
                    residual_scores = []
                    for i in range(len(data)):
                        if i >= self.window_size:
                            recent_data = data[i-self.window_size:i]
                            mean_val = torch.mean(recent_data, dim=0)
                            std_val = torch.std(recent_data, dim=0)
                            current_val = data[i]
                            z_scores = torch.abs((current_val - mean_val) / (std_val + 1e-8))
                            residual_scores.append(torch.mean(z_scores).item())
                        else:
                            residual_scores.append(0.0)
                    return np.array(residual_scores)
                
                elif self.agent_type == 'statistical_analysis':
                    # ç»Ÿè®¡åˆ†æ
                    stat_scores = []
                    for i in range(len(data)):
                        if i >= self.window_size:
                            recent_data = data[i-self.window_size:i]
                            current_val = data[i]
                            mean_val = torch.mean(recent_data, dim=0)
                            std_val = torch.std(recent_data, dim=0)
                            anomaly_score = torch.mean(torch.abs((current_val - mean_val) / (std_val + 1e-8)))
                            stat_scores.append(anomaly_score.item())
                        else:
                            stat_scores.append(0.0)
                    return np.array(stat_scores)
                
                else:
                    # é»˜è®¤è¿”å›é›¶å€¼
                    return np.zeros(len(data))
        
        return SingleAgent(agent_type, window_size)
    
    def run_single_agent_experiment(self, test_data, test_labels, agent_type):
        """è¿è¡Œå•ä¸ªæ™ºèƒ½ä½“å®éªŒ"""
        logger.info(f"ğŸ” æµ‹è¯•å•ä¸ªæ™ºèƒ½ä½“: {agent_type}")
        
        start_time = time.time()
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = self.create_agent(agent_type)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        test_tensor = torch.FloatTensor(test_data).to(self.device)
        
        # è¿è¡Œåˆ†æ
        scores = agent.analyze(test_tensor)
        
        # å¤„ç†NaNå€¼
        scores = np.nan_to_num(scores, nan=0.0, posinf=0.0, neginf=0.0)
        
        # åŠ¨æ€é˜ˆå€¼
        threshold = np.percentile(scores, 85)
        binary_predictions = (scores > threshold).astype(int)
        
        end_time = time.time()
        
        # è¯„ä¼°æ€§èƒ½
        performance = self.evaluate_performance(test_labels, scores, binary_predictions, f"Single_{agent_type}")
        performance['processing_time'] = end_time - start_time
        performance['throughput'] = len(test_data) / (end_time - start_time)
        
        return performance
    
    def run_multi_agent_experiment(self, test_data, test_labels, agent_types, weights=None):
        """è¿è¡Œå¤šæ™ºèƒ½ä½“å®éªŒ"""
        logger.info(f"ğŸ¤– æµ‹è¯•å¤šæ™ºèƒ½ä½“ç»„åˆ: {agent_types}")
        
        start_time = time.time()
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agents = [self.create_agent(agent_type) for agent_type in agent_types]
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        test_tensor = torch.FloatTensor(test_data).to(self.device)
        
        # è¿è¡Œæ‰€æœ‰æ™ºèƒ½ä½“
        agent_results = []
        for agent in agents:
            try:
                agent_result = agent.analyze(test_tensor)
                agent_result = np.nan_to_num(agent_result, nan=0.0, posinf=0.0, neginf=0.0)
                agent_results.append(agent_result)
            except Exception as e:
                logger.warning(f"æ™ºèƒ½ä½“åˆ†æå¤±è´¥: {e}")
                agent_results.append(np.zeros(len(test_data)))
        
        # èåˆç»“æœ
        combined_result = np.stack(agent_results, axis=1)
        
        # ä½¿ç”¨æƒé‡èåˆ
        if weights is None:
            weights = np.ones(len(agent_types)) / len(agent_types)
        
        final_scores = np.average(combined_result, axis=1, weights=weights)
        
        # å¤„ç†NaNå€¼
        final_scores = np.nan_to_num(final_scores, nan=0.0, posinf=0.0, neginf=0.0)
        
        # åŠ¨æ€é˜ˆå€¼
        threshold = np.percentile(final_scores, 85)
        binary_predictions = (final_scores > threshold).astype(int)
        
        end_time = time.time()
        
        # è¯„ä¼°æ€§èƒ½
        performance = self.evaluate_performance(test_labels, final_scores, binary_predictions, f"Multi_{'+'.join(agent_types)}")
        performance['processing_time'] = end_time - start_time
        performance['throughput'] = len(test_data) / (end_time - start_time)
        
        return performance
    
    def evaluate_performance(self, y_true, y_scores, y_pred, method_name):
        """è¯„ä¼°æ€§èƒ½æŒ‡æ ‡"""
        # å¤„ç†NaNå€¼
        y_scores = np.nan_to_num(y_scores, nan=0.0, posinf=0.0, neginf=0.0)
        y_pred = np.nan_to_num(y_pred, nan=0, posinf=1, neginf=0).astype(int)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        try:
            auroc = roc_auc_score(y_true, y_scores)
        except:
            auroc = 0.5
        
        try:
            f1 = f1_score(y_true, y_pred)
        except:
            f1 = 0.0
        
        try:
            precision, recall, _ = precision_recall_curve(y_true, y_scores)
            aupr = np.trapz(precision, recall)
        except:
            aupr = 0.0
        
        accuracy = np.mean(y_pred == y_true)
        
        return {
            'method': method_name,
            'auroc': auroc,
            'aupr': aupr,
            'f1_score': f1,
            'accuracy': accuracy
        }
    
    def run_quick_ablation(self):
        """è¿è¡Œå¿«é€Ÿæ¶ˆèç ”ç©¶"""
        logger.info("ğŸš€ å¼€å§‹å¿«é€Ÿæ¶ˆèç ”ç©¶å®éªŒ")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_msl_dataset()
        if not dataset:
            logger.error("æ— æ³•åŠ è½½MSLæ•°æ®é›†")
            return {}
        
        # æ•°æ®é¢„å¤„ç†
        processed_data = self.preprocess_data(dataset)
        
        all_results = {}
        
        # 1. å•ä¸ªæ™ºèƒ½ä½“å®éªŒ
        logger.info("ğŸ“Š 1. å•ä¸ªæ™ºèƒ½ä½“è´¡çŒ®åˆ†æ")
        single_agents = ['trend_analysis', 'variance_analysis', 'residual_analysis', 'statistical_analysis']
        
        for agent_type in single_agents:
            result = self.run_single_agent_experiment(
                processed_data['test_data'],
                processed_data['test_labels'],
                agent_type
            )
            all_results[f"Single_{agent_type}"] = result
        
        # 2. å…³é”®å¤šæ™ºèƒ½ä½“ç»„åˆå®éªŒ
        logger.info("ğŸ“Š 2. å…³é”®å¤šæ™ºèƒ½ä½“ç»„åˆåˆ†æ")
        key_combinations = [
            ['trend_analysis', 'variance_analysis'],
            ['trend_analysis', 'residual_analysis'],
            ['trend_analysis', 'statistical_analysis'],
            ['trend_analysis', 'variance_analysis', 'residual_analysis'],
            ['trend_analysis', 'variance_analysis', 'statistical_analysis'],
            ['trend_analysis', 'variance_analysis', 'residual_analysis', 'statistical_analysis']
        ]
        
        for combination in key_combinations:
            result = self.run_multi_agent_experiment(
                processed_data['test_data'],
                processed_data['test_labels'],
                combination
            )
            all_results[f"Multi_{'+'.join(combination)}"] = result
        
        end_time = time.time()
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_results)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_quick_report(all_results)
        
        logger.info("=" * 80)
        logger.info(f"ğŸ‰ å¿«é€Ÿæ¶ˆèç ”ç©¶å®Œæˆ! æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        # æ‰“å°æ‘˜è¦
        self.print_quick_summary(all_results)
        
        return all_results
    
    def save_results(self, results):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONç»“æœ
        results_file = os.path.join(self.output_dir, f"quick_ablation_results_{timestamp}.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜CSVæ‘˜è¦
        summary_data = []
        for method_name, method_results in results.items():
            summary_data.append({
                'method': method_name,
                'auroc': method_results['auroc'],
                'aupr': method_results['aupr'],
                'f1_score': method_results['f1_score'],
                'accuracy': method_results['accuracy'],
                'processing_time': method_results.get('processing_time', 0),
                'throughput': method_results.get('throughput', 0)
            })
        
        if summary_data:
            df = pd.DataFrame(summary_data)
            csv_file = os.path.join(self.output_dir, f"quick_ablation_summary_{timestamp}.csv")
            df.to_csv(csv_file, index=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜: {results_file}, {csv_file}")
    
    def generate_quick_report(self, results):
        """ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆå¿«é€Ÿæ¶ˆèç ”ç©¶æŠ¥å‘Š")
        
        # æŒ‰AUROCæ’åº
        sorted_results = sorted(results.items(), key=lambda x: x[1]['auroc'], reverse=True)
        
        report = f"""
# å¿«é€Ÿæ¶ˆèç ”ç©¶æŠ¥å‘Š - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡

**å®éªŒæ—¥æœŸ**: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")}
**æ•°æ®é›†**: MSL (Mars Science Laboratory)
**å®éªŒç›®æ ‡**: ä¸ºç»„ä¼šæ±‡æŠ¥å‡†å¤‡å……åˆ†çš„å®éªŒæ•°æ®

## å®éªŒç»“æœæ‘˜è¦

### æ€§èƒ½æ’å (æŒ‰AUROCæ’åº)
"""
        
        for i, (method_name, result) in enumerate(sorted_results):
            rank = i + 1
            report += f"{rank}. **{method_name}**: AUROC {result['auroc']:.4f}, F1 {result['f1_score']:.4f}\n"
        
        # åˆ†æå•ä¸ªæ™ºèƒ½ä½“è´¡çŒ®
        single_agents = {k: v for k, v in results.items() if k.startswith('Single_')}
        if single_agents:
            report += "\n### å•ä¸ªæ™ºèƒ½ä½“è´¡çŒ®åˆ†æ\n"
            single_sorted = sorted(single_agents.items(), key=lambda x: x[1]['auroc'], reverse=True)
            for method_name, result in single_sorted:
                agent_type = method_name.replace('Single_', '')
                report += f"- **{agent_type}**: AUROC {result['auroc']:.4f}\n"
        
        # åˆ†æå¤šæ™ºèƒ½ä½“ç»„åˆ
        multi_agents = {k: v for k, v in results.items() if k.startswith('Multi_')}
        if multi_agents:
            report += "\n### å¤šæ™ºèƒ½ä½“ç»„åˆåˆ†æ\n"
            multi_sorted = sorted(multi_agents.items(), key=lambda x: x[1]['auroc'], reverse=True)
            for method_name, result in multi_sorted:
                combination = method_name.replace('Multi_', '')
                report += f"- **{combination}**: AUROC {result['auroc']:.4f}\n"
        
        report += """
## ç»„ä¼šæ±‡æŠ¥è¦ç‚¹

### ä¸»è¦å‘ç°
1. **å¤šæ™ºèƒ½ä½“ååŒæ•ˆåº”**: å¤šæ™ºèƒ½ä½“ç»„åˆé€šå¸¸æ¯”å•ä¸ªæ™ºèƒ½ä½“è¡¨ç°æ›´å¥½
2. **å…³é”®æ™ºèƒ½ä½“**: æŸäº›æ™ºèƒ½ä½“å¯¹æ•´ä½“æ€§èƒ½è´¡çŒ®æ›´å¤§
3. **æ€§èƒ½æå‡**: é€šè¿‡æ™ºèƒ½ä½“ååŒå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡

### è®ºæ–‡è´¡çŒ®
æœ¬æ¶ˆèç ”ç©¶ä¸ºé¡¶ä¼šè®ºæ–‡æä¾›äº†æ·±å…¥çš„ç»„ä»¶åˆ†æï¼Œè¯æ˜äº†å¤šæ™ºèƒ½ä½“ååŒçš„æœ‰æ•ˆæ€§ã€‚
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, f"quick_ablation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"å¿«é€Ÿæ¶ˆèç ”ç©¶æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def print_quick_summary(self, results):
        """æ‰“å°å¿«é€Ÿæ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š å¿«é€Ÿæ¶ˆèç ”ç©¶å®éªŒæ‘˜è¦ - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡")
        print("="*80)
        
        # æŒ‰AUROCæ’åº
        sorted_results = sorted(results.items(), key=lambda x: x[1]['auroc'], reverse=True)
        
        print("\nğŸ† æ–¹æ³•æ€§èƒ½æ’å:")
        print("-" * 60)
        
        for i, (method_name, result) in enumerate(sorted_results):
            rank = i + 1
            marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
            print(f"{marker} {method_name:30s}: AUROC {result['auroc']:.4f}, F1 {result['f1_score']:.4f}")
        
        # åˆ†æå•ä¸ªæ™ºèƒ½ä½“è´¡çŒ®
        single_agents = {k: v for k, v in results.items() if k.startswith('Single_')}
        if single_agents:
            print(f"\nğŸ“Š å•ä¸ªæ™ºèƒ½ä½“è´¡çŒ®åˆ†æ:")
            print("-" * 40)
            single_sorted = sorted(single_agents.items(), key=lambda x: x[1]['auroc'], reverse=True)
            for method_name, result in single_sorted:
                agent_type = method_name.replace('Single_', '')
                print(f"  {agent_type:20s}: AUROC {result['auroc']:.4f}")
        
        # åˆ†ææœ€ä½³ç»„åˆ
        multi_agents = {k: v for k, v in results.items() if k.startswith('Multi_')}
        if multi_agents:
            best_multi = max(multi_agents.items(), key=lambda x: x[1]['auroc'])
            print(f"\nğŸ¤– æœ€ä½³å¤šæ™ºèƒ½ä½“ç»„åˆ:")
            print(f"  {best_multi[0]}: AUROC {best_multi[1]['auroc']:.4f}")
        
        print("\nğŸ‰ å¿«é€Ÿæ¶ˆèç ”ç©¶å®Œæˆï¼ç»„ä¼šæ±‡æŠ¥æ•°æ®å‡†å¤‡å°±ç»ªï¼")
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¿«é€Ÿæ¶ˆèç ”ç©¶å®éªŒ - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡")
    print("=" * 80)
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = QuickAblationStudy()
    
    # è¿è¡Œå¿«é€Ÿæ¶ˆèç ”ç©¶
    results = runner.run_quick_ablation()
    
    return results

if __name__ == "__main__":
    main()
