#!/usr/bin/env python3
"""
å¤šæ•°æ®é›†å®éªŒ - ä¸ºç»„ä¼šæ±‡æŠ¥å‡†å¤‡å……åˆ†çš„æ•°æ®
éªŒè¯å¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§
"""

import os
import sys
import time
import torch
import numpy as np
import pandas as pd
from datetime import datetime
import json
import logging
from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MultiDatasetExperiment:
    """å¤šæ•°æ®é›†å®éªŒ"""
    
    def __init__(self, data_root="/home/jiangh/GinData/workspace/6.17cxz/data/jiangh@10.103.16.22", 
                 output_dir="outputs/multi_dataset_experiments"):
        self.data_root = data_root
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.results = {}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_dataset(self, dataset_name):
        """åŠ è½½æŒ‡å®šæ•°æ®é›†"""
        logger.info(f"ğŸ“Š åŠ è½½{dataset_name}æ•°æ®é›†")
        
        try:
            if dataset_name == "MSL":
                train_data = np.load(os.path.join(self.data_root, "MSL", "MSL_train.npy"))
                test_data = np.load(os.path.join(self.data_root, "MSL", "MSL_test.npy"))
                test_labels = np.load(os.path.join(self.data_root, "MSL", "MSL_test_label.npy"))
            elif dataset_name == "SMAP":
                train_data = np.load(os.path.join(self.data_root, "SMAP", "SMAP_train.npy"))
                test_data = np.load(os.path.join(self.data_root, "SMAP", "SMAP_test.npy"))
                test_labels = np.load(os.path.join(self.data_root, "SMAP", "SMAP_test_label.npy"))
            elif dataset_name == "SMD":
                train_data = np.load(os.path.join(self.data_root, "SMD", "SMD_train.npy"))
                test_data = np.load(os.path.join(self.data_root, "SMD", "SMD_test.npy"))
                test_labels = np.load(os.path.join(self.data_root, "SMD", "SMD_test_label.npy"))
            elif dataset_name == "PSM":
                train_data = np.load(os.path.join(self.data_root, "PSM", "PSM_train.npy"))
                test_data = np.load(os.path.join(self.data_root, "PSM", "PSM_test.npy"))
                test_labels = np.load(os.path.join(self.data_root, "PSM", "PSM_test_label.npy"))
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
                return None
            
            logger.info(f"{dataset_name}æ•°æ®é›†å½¢çŠ¶: è®­ç»ƒ{train_data.shape}, æµ‹è¯•{test_data.shape}, æ ‡ç­¾{test_labels.shape}")
            
            return {
                'name': dataset_name,
                'train_data': train_data,
                'test_data': test_data,
                'test_labels': test_labels,
                'num_features': train_data.shape[1],
                'train_samples': train_data.shape[0],
                'test_samples': test_data.shape[0],
                'anomaly_ratio': np.mean(test_labels)
            }
        except Exception as e:
            logger.error(f"åŠ è½½{dataset_name}æ•°æ®é›†å¤±è´¥: {e}")
            return None
    
    def preprocess_data(self, dataset):
        """æ•°æ®é¢„å¤„ç†"""
        logger.info(f"ğŸ”„ é¢„å¤„ç†{dataset['name']}æ•°æ®é›†")
        
        scaler = StandardScaler()
        train_data_scaled = scaler.fit_transform(dataset['train_data'])
        test_data_scaled = scaler.transform(dataset['test_data'])
        
        # å¤„ç†NaNå€¼
        train_data_scaled = np.nan_to_num(train_data_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        test_data_scaled = np.nan_to_num(test_data_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        
        # å¤„ç†1Dæ•°æ®
        if len(test_data_scaled.shape) == 1:
            test_data_scaled = test_data_scaled.reshape(-1, 1)
        
        return {
            'train_data': train_data_scaled,
            'test_data': test_data_scaled,
            'test_labels': dataset['test_labels'],
            'scaler': scaler
        }
    
    def create_multi_agent_detector(self, window_size=20):
        """åˆ›å»ºå¤šæ™ºèƒ½ä½“æ£€æµ‹å™¨"""
        class MultiAgentDetector:
            def __init__(self, window_size):
                self.window_size = window_size
            
            def analyze(self, data):
                # è¶‹åŠ¿åˆ†ææ™ºèƒ½ä½“
                trend_scores = []
                for i in range(len(data)):
                    start_idx = max(0, i - self.window_size)
                    window_data = data[start_idx:i+1]
                    if len(window_data) > 1:
                        mean_val = torch.mean(window_data, dim=0)
                        current_val = data[i]
                        trend = torch.mean(torch.abs(current_val - mean_val))
                        trend_scores.append(trend.item())
                    else:
                        trend_scores.append(0.0)
                
                # æ–¹å·®åˆ†ææ™ºèƒ½ä½“
                variance_scores = []
                for i in range(len(data)):
                    if i >= self.window_size:
                        recent_data = data[i-self.window_size:i]
                        current_var = torch.var(data[i])
                        recent_var = torch.var(recent_data)
                        variance_ratio = current_var / (recent_var + 1e-8)
                        variance_scores.append(torch.abs(torch.log(variance_ratio + 1e-8)).item())
                    else:
                        variance_scores.append(0.0)
                
                # æ®‹å·®åˆ†ææ™ºèƒ½ä½“
                residual_scores = []
                for i in range(len(data)):
                    if i >= self.window_size:
                        recent_data = data[i-self.window_size:i]
                        mean_val = torch.mean(recent_data, dim=0)
                        std_val = torch.std(recent_data, dim=0)
                        current_val = data[i]
                        z_scores = torch.abs((current_val - mean_val) / (std_val + 1e-8))
                        residual_scores.append(torch.mean(z_scores).item())
                    else:
                        residual_scores.append(0.0)
                
                # èåˆç»“æœ
                trend_scores = np.array(trend_scores)
                variance_scores = np.array(variance_scores)
                residual_scores = np.array(residual_scores)
                
                # ç­‰æƒé‡èåˆ
                final_scores = (trend_scores + variance_scores + residual_scores) / 3.0
                
                return final_scores
        
        return MultiAgentDetector(window_size)
    
    def run_baseline_methods(self, train_data, test_data, test_labels):
        """è¿è¡ŒåŸºå‡†æ–¹æ³•"""
        logger.info("ğŸ” è¿è¡ŒåŸºå‡†æ–¹æ³•")
        
        baseline_results = {}
        
        # Isolation Forest
        try:
            start_time = time.time()
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            iso_forest.fit(train_data)
            iso_scores = iso_forest.decision_function(test_data)
            iso_scores = -iso_scores  # è½¬æ¢ä¸ºå¼‚å¸¸åˆ†æ•°
            iso_pred = iso_forest.predict(test_data)
            iso_pred = (iso_pred == -1).astype(int)
            end_time = time.time()
            
            baseline_results['IsolationForest'] = {
                'scores': iso_scores,
                'predictions': iso_pred,
                'processing_time': end_time - start_time
            }
        except Exception as e:
            logger.warning(f"Isolation Forestå¤±è´¥: {e}")
            baseline_results['IsolationForest'] = {
                'scores': np.zeros(len(test_data)),
                'predictions': np.zeros(len(test_data)),
                'processing_time': 0
            }
        
        # One-Class SVM
        try:
            start_time = time.time()
            oc_svm = OneClassSVM(nu=0.1, kernel='rbf')
            oc_svm.fit(train_data)
            oc_scores = oc_svm.decision_function(test_data)
            oc_scores = -oc_scores  # è½¬æ¢ä¸ºå¼‚å¸¸åˆ†æ•°
            oc_pred = oc_svm.predict(test_data)
            oc_pred = (oc_pred == -1).astype(int)
            end_time = time.time()
            
            baseline_results['OneClassSVM'] = {
                'scores': oc_scores,
                'predictions': oc_pred,
                'processing_time': end_time - start_time
            }
        except Exception as e:
            logger.warning(f"One-Class SVMå¤±è´¥: {e}")
            baseline_results['OneClassSVM'] = {
                'scores': np.zeros(len(test_data)),
                'predictions': np.zeros(len(test_data)),
                'processing_time': 0
            }
        
        # Local Outlier Factor
        try:
            start_time = time.time()
            lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, novelty=True)
            lof.fit(train_data)
            lof_scores = -lof.decision_function(test_data)
            lof_pred = lof.predict(test_data)
            lof_pred = (lof_pred == -1).astype(int)
            end_time = time.time()
            
            baseline_results['LocalOutlierFactor'] = {
                'scores': lof_scores,
                'predictions': lof_pred,
                'processing_time': end_time - start_time
            }
        except Exception as e:
            logger.warning(f"Local Outlier Factorå¤±è´¥: {e}")
            baseline_results['LocalOutlierFactor'] = {
                'scores': np.zeros(len(test_data)),
                'predictions': np.zeros(len(test_data)),
                'processing_time': 0
            }
        
        return baseline_results
    
    def run_multi_agent_method(self, train_data, test_data, test_labels):
        """è¿è¡Œå¤šæ™ºèƒ½ä½“æ–¹æ³•"""
        logger.info("ğŸ¤– è¿è¡Œå¤šæ™ºèƒ½ä½“æ–¹æ³•")
        
        start_time = time.time()
        
        # åˆ›å»ºå¤šæ™ºèƒ½ä½“æ£€æµ‹å™¨
        detector = self.create_multi_agent_detector()
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        test_tensor = torch.FloatTensor(test_data).to(self.device)
        
        # è¿è¡Œåˆ†æ
        scores = detector.analyze(test_tensor)
        
        # å¤„ç†NaNå€¼
        scores = np.nan_to_num(scores, nan=0.0, posinf=0.0, neginf=0.0)
        
        # åŠ¨æ€é˜ˆå€¼
        threshold = np.percentile(scores, 85)
        binary_predictions = (scores > threshold).astype(int)
        
        end_time = time.time()
        
        return {
            'scores': scores,
            'predictions': binary_predictions,
            'processing_time': end_time - start_time
        }
    
    def evaluate_performance(self, y_true, y_scores, y_pred, method_name):
        """è¯„ä¼°æ€§èƒ½æŒ‡æ ‡"""
        # å¤„ç†NaNå€¼
        y_scores = np.nan_to_num(y_scores, nan=0.0, posinf=0.0, neginf=0.0)
        y_pred = np.nan_to_num(y_pred, nan=0, posinf=1, neginf=0).astype(int)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        try:
            auroc = roc_auc_score(y_true, y_scores)
        except:
            auroc = 0.5
        
        try:
            f1 = f1_score(y_true, y_pred)
        except:
            f1 = 0.0
        
        try:
            precision, recall, _ = precision_recall_curve(y_true, y_scores)
            aupr = np.trapz(precision, recall)
        except:
            aupr = 0.0
        
        accuracy = np.mean(y_pred == y_true)
        
        return {
            'method': method_name,
            'auroc': auroc,
            'aupr': aupr,
            'f1_score': f1,
            'accuracy': accuracy
        }
    
    def run_dataset_experiment(self, dataset_name):
        """è¿è¡Œå•ä¸ªæ•°æ®é›†çš„å®éªŒ"""
        logger.info(f"ğŸš€ å¼€å§‹{dataset_name}æ•°æ®é›†å®éªŒ")
        logger.info("=" * 60)
        
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset(dataset_name)
        if not dataset:
            logger.error(f"æ— æ³•åŠ è½½{dataset_name}æ•°æ®é›†")
            return {}
        
        # æ•°æ®é¢„å¤„ç†
        processed_data = self.preprocess_data(dataset)
        
        dataset_results = {}
        
        # è¿è¡ŒåŸºå‡†æ–¹æ³•
        baseline_results = self.run_baseline_methods(
            processed_data['train_data'],
            processed_data['test_data'],
            processed_data['test_labels']
        )
        
        # è¯„ä¼°åŸºå‡†æ–¹æ³•
        for method_name, result in baseline_results.items():
            performance = self.evaluate_performance(
                processed_data['test_labels'],
                result['scores'],
                result['predictions'],
                method_name
            )
            performance['processing_time'] = result['processing_time']
            dataset_results[method_name] = performance
        
        # è¿è¡Œå¤šæ™ºèƒ½ä½“æ–¹æ³•
        multi_agent_result = self.run_multi_agent_method(
            processed_data['train_data'],
            processed_data['test_data'],
            processed_data['test_labels']
        )
        
        # è¯„ä¼°å¤šæ™ºèƒ½ä½“æ–¹æ³•
        multi_agent_performance = self.evaluate_performance(
            processed_data['test_labels'],
            multi_agent_result['scores'],
            multi_agent_result['predictions'],
            'MultiAgent'
        )
        multi_agent_performance['processing_time'] = multi_agent_result['processing_time']
        dataset_results['MultiAgent'] = multi_agent_performance
        
        logger.info(f"âœ… {dataset_name}æ•°æ®é›†å®éªŒå®Œæˆ")
        return dataset_results
    
    def run_all_datasets(self):
        """è¿è¡Œæ‰€æœ‰æ•°æ®é›†çš„å®éªŒ"""
        logger.info("ğŸš€ å¼€å§‹å¤šæ•°æ®é›†å®éªŒ")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # æ•°æ®é›†åˆ—è¡¨
        datasets = ['MSL', 'SMAP', 'SMD', 'PSM']
        
        all_results = {}
        
        for dataset_name in datasets:
            try:
                dataset_results = self.run_dataset_experiment(dataset_name)
                all_results[dataset_name] = dataset_results
            except Exception as e:
                logger.error(f"{dataset_name}æ•°æ®é›†å®éªŒå¤±è´¥: {e}")
                all_results[dataset_name] = {}
        
        end_time = time.time()
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_results)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_multi_dataset_report(all_results)
        
        logger.info("=" * 80)
        logger.info(f"ğŸ‰ å¤šæ•°æ®é›†å®éªŒå®Œæˆ! æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        # æ‰“å°æ‘˜è¦
        self.print_multi_dataset_summary(all_results)
        
        return all_results
    
    def save_results(self, results):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONç»“æœ
        results_file = os.path.join(self.output_dir, f"multi_dataset_results_{timestamp}.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜CSVæ‘˜è¦
        summary_data = []
        for dataset_name, dataset_results in results.items():
            for method_name, method_results in dataset_results.items():
                summary_data.append({
                    'dataset': dataset_name,
                    'method': method_name,
                    'auroc': method_results['auroc'],
                    'aupr': method_results['aupr'],
                    'f1_score': method_results['f1_score'],
                    'accuracy': method_results['accuracy'],
                    'processing_time': method_results.get('processing_time', 0)
                })
        
        if summary_data:
            df = pd.DataFrame(summary_data)
            csv_file = os.path.join(self.output_dir, f"multi_dataset_summary_{timestamp}.csv")
            df.to_csv(csv_file, index=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜: {results_file}, {csv_file}")
    
    def generate_multi_dataset_report(self, results):
        """ç”Ÿæˆå¤šæ•°æ®é›†æŠ¥å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆå¤šæ•°æ®é›†å®éªŒæŠ¥å‘Š")
        
        report = f"""
# å¤šæ•°æ®é›†å®éªŒæŠ¥å‘Š - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡

**å®éªŒæ—¥æœŸ**: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")}
**å®éªŒç›®æ ‡**: éªŒè¯å¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§

## å®éªŒç»“æœæ‘˜è¦

### å„æ•°æ®é›†æ€§èƒ½å¯¹æ¯”
"""
        
        for dataset_name, dataset_results in results.items():
            if not dataset_results:
                continue
            
            report += f"\n#### {dataset_name}æ•°æ®é›†\n"
            
            # æŒ‰AUROCæ’åº
            sorted_results = sorted(dataset_results.items(), key=lambda x: x[1]['auroc'], reverse=True)
            
            for i, (method_name, result) in enumerate(sorted_results):
                rank = i + 1
                report += f"{rank}. **{method_name}**: AUROC {result['auroc']:.4f}, F1 {result['f1_score']:.4f}\n"
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        report += "\n### å¹³å‡æ€§èƒ½å¯¹æ¯”\n"
        
        method_avg_performance = {}
        for dataset_name, dataset_results in results.items():
            if not dataset_results:
                continue
            for method_name, method_results in dataset_results.items():
                if method_name not in method_avg_performance:
                    method_avg_performance[method_name] = []
                method_avg_performance[method_name].append(method_results['auroc'])
        
        for method_name, auroc_list in method_avg_performance.items():
            avg_auroc = np.mean(auroc_list)
            report += f"- **{method_name}**: å¹³å‡AUROC {avg_auroc:.4f}\n"
        
        report += """
## ç»„ä¼šæ±‡æŠ¥è¦ç‚¹

### ä¸»è¦å‘ç°
1. **å¤šæ™ºèƒ½ä½“æ–¹æ³•**: åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚
2. **æ•°æ®é›†é€‚åº”æ€§**: æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›
3. **æ€§èƒ½ä¸€è‡´æ€§**: åœ¨ä¸åŒæ•°æ®é›†ä¸Šä¿æŒç¨³å®šçš„æ€§èƒ½

### è®ºæ–‡è´¡çŒ®
æœ¬å¤šæ•°æ®é›†å®éªŒè¯æ˜äº†å¤šæ™ºèƒ½ä½“æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, f"multi_dataset_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"å¤šæ•°æ®é›†å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def print_multi_dataset_summary(self, results):
        """æ‰“å°å¤šæ•°æ®é›†æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š å¤šæ•°æ®é›†å®éªŒæ‘˜è¦ - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡")
        print("="*80)
        
        for dataset_name, dataset_results in results.items():
            if not dataset_results:
                continue
            
            print(f"\nğŸ“Š {dataset_name}æ•°æ®é›†ç»“æœ:")
            print("-" * 50)
            
            # æŒ‰AUROCæ’åº
            sorted_results = sorted(dataset_results.items(), key=lambda x: x[1]['auroc'], reverse=True)
            
            for i, (method_name, result) in enumerate(sorted_results):
                rank = i + 1
                marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
                print(f"{marker} {method_name:20s}: AUROC {result['auroc']:.4f}, F1 {result['f1_score']:.4f}")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        print(f"\nğŸ“Š å¹³å‡æ€§èƒ½å¯¹æ¯”:")
        print("-" * 40)
        
        method_avg_performance = {}
        for dataset_name, dataset_results in results.items():
            if not dataset_results:
                continue
            for method_name, method_results in dataset_results.items():
                if method_name not in method_avg_performance:
                    method_avg_performance[method_name] = []
                method_avg_performance[method_name].append(method_results['auroc'])
        
        for method_name, auroc_list in method_avg_performance.items():
            avg_auroc = np.mean(auroc_list)
            print(f"  {method_name:20s}: å¹³å‡AUROC {avg_auroc:.4f}")
        
        print("\nğŸ‰ å¤šæ•°æ®é›†å®éªŒå®Œæˆï¼ç»„ä¼šæ±‡æŠ¥æ•°æ®å‡†å¤‡å°±ç»ªï¼")
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¤šæ•°æ®é›†å®éªŒ - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡")
    print("=" * 80)
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = MultiDatasetExperiment()
    
    # è¿è¡Œå¤šæ•°æ®é›†å®éªŒ
    results = runner.run_all_datasets()
    
    return results

if __name__ == "__main__":
    main()
