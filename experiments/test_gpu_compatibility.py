#!/usr/bin/env python3
"""
GPUå…¼å®¹æ€§æµ‹è¯•è„šæœ¬
ç¡®ä¿åœ¨æœåŠ¡å™¨ä¸Šèƒ½æ­£ç¡®ä½¿ç”¨GPUè¿è¡Œ
"""

import torch
import sys
import os
import time

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    print("ğŸ§ª æµ‹è¯•1: GPUå¯ç”¨æ€§æµ‹è¯•")
    print("-" * 50)
    
    print(f"ğŸ“Š PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ“Š CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"ğŸ“Š CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"ğŸ“Š GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            print(f"ğŸ“Š GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"ğŸ“Š GPU {i} å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")
            print(f"ğŸ“Š GPU {i} è®¡ç®—èƒ½åŠ›: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}")
        
        return True
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œéœ€è¦å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
        return False

def test_gpu_tensor_operations():
    """æµ‹è¯•GPUå¼ é‡æ“ä½œ"""
    print("\nğŸ§ª æµ‹è¯•2: GPUå¼ é‡æ“ä½œæµ‹è¯•")
    print("-" * 50)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
        return True
    
    try:
        # åˆ›å»ºGPUå¼ é‡
        device = torch.device('cuda')
        print(f"ğŸ“Š ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æµ‹è¯•åŸºæœ¬å¼ é‡æ“ä½œ
        a = torch.randn(1000, 1000).to(device)
        b = torch.randn(1000, 1000).to(device)
        
        # çŸ©é˜µä¹˜æ³•
        start_time = time.time()
        c = torch.matmul(a, b)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"âœ… GPUçŸ©é˜µä¹˜æ³•æˆåŠŸ: {c.shape}")
        print(f"ğŸ“Š GPUè®¡ç®—æ—¶é—´: {gpu_time:.4f} ç§’")
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        memory_allocated = torch.cuda.memory_allocated() / 1e6
        memory_reserved = torch.cuda.memory_reserved() / 1e6
        print(f"ğŸ“Š GPUå†…å­˜åˆ†é…: {memory_allocated:.1f} MB")
        print(f"ğŸ“Š GPUå†…å­˜ä¿ç•™: {memory_reserved:.1f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUå¼ é‡æ“ä½œå¤±è´¥: {e}")
        return False

def test_multi_agent_gpu():
    """æµ‹è¯•å¤šæ™ºèƒ½ä½“ç³»ç»ŸGPUè¿è¡Œ"""
    print("\nğŸ§ª æµ‹è¯•3: å¤šæ™ºèƒ½ä½“ç³»ç»ŸGPUæµ‹è¯•")
    print("-" * 50)
    
    try:
        from agents.multi_agent_detector import MultiAgentAnomalyDetector
        
        # åˆ›å»ºæ£€æµ‹å™¨
        config = {'trend_agent': {}}
        detector = MultiAgentAnomalyDetector(config)
        
        # æµ‹è¯•æ•°æ®
        if torch.cuda.is_available():
            device = torch.device('cuda')
            test_data = torch.randn(1, 3, 64, 64).to(device)
            print(f"ğŸ“Š æµ‹è¯•æ•°æ®è®¾å¤‡: {test_data.device}")
        else:
            test_data = torch.randn(1, 3, 64, 64)
            print("ğŸ“Š ä½¿ç”¨CPUæµ‹è¯•æ•°æ®")
        
        # æ‰§è¡Œæ£€æµ‹
        start_time = time.time()
        result = detector.detect_anomaly(test_data)
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        gpu_time = time.time() - start_time
        
        if 'error' in result:
            print(f"âŒ æ£€æµ‹å¤±è´¥: {result['error']}")
            return False
        
        print(f"âœ… å¼‚å¸¸æ£€æµ‹æˆåŠŸ: å¼‚å¸¸åˆ†æ•°={result['final_decision']['anomaly_score']:.3f}")
        print(f"ğŸ“Š æ£€æµ‹æ—¶é—´: {gpu_time:.4f} ç§’")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šæ™ºèƒ½ä½“GPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gpu_memory_management():
    """æµ‹è¯•GPUå†…å­˜ç®¡ç†"""
    print("\nğŸ§ª æµ‹è¯•4: GPUå†…å­˜ç®¡ç†æµ‹è¯•")
    print("-" * 50)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå†…å­˜æµ‹è¯•")
        return True
    
    try:
        device = torch.device('cuda')
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = torch.cuda.memory_allocated()
        print(f"ğŸ“Š åˆå§‹GPUå†…å­˜: {initial_memory / 1e6:.1f} MB")
        
        # åˆ›å»ºå¤§é‡å¼ é‡
        tensors = []
        for i in range(10):
            tensor = torch.randn(1000, 1000).to(device)
            tensors.append(tensor)
        
        # è®°å½•å½“å‰å†…å­˜
        current_memory = torch.cuda.memory_allocated()
        print(f"ğŸ“Š åˆ›å»ºå¼ é‡åå†…å­˜: {current_memory / 1e6:.1f} MB")
        print(f"ğŸ“Š å†…å­˜å¢é•¿: {(current_memory - initial_memory) / 1e6:.1f} MB")
        
        # æ¸…ç†å†…å­˜
        del tensors
        torch.cuda.empty_cache()
        
        # è®°å½•æ¸…ç†åå†…å­˜
        final_memory = torch.cuda.memory_allocated()
        print(f"ğŸ“Š æ¸…ç†åå†…å­˜: {final_memory / 1e6:.1f} MB")
        
        # æ£€æŸ¥å†…å­˜æ˜¯å¦è¢«æ­£ç¡®é‡Šæ”¾
        if final_memory <= initial_memory * 1.1:  # å…è®¸10%çš„è¯¯å·®
            print("âœ… GPUå†…å­˜ç®¡ç†æ­£å¸¸")
            return True
        else:
            print("âš ï¸  GPUå†…å­˜å¯èƒ½æœªå®Œå…¨é‡Šæ”¾")
            return False
        
    except Exception as e:
        print(f"âŒ GPUå†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    print("\nğŸ§ª æµ‹è¯•5: GPUæ€§èƒ½æµ‹è¯•")
    print("-" * 50)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæ€§èƒ½æµ‹è¯•")
        return True
    
    try:
        device = torch.device('cuda')
        
        # æ€§èƒ½æµ‹è¯•
        sizes = [100, 500, 1000, 2000]
        
        for size in sizes:
            # GPUæµ‹è¯•
            a_gpu = torch.randn(size, size).to(device)
            b_gpu = torch.randn(size, size).to(device)
            
            start_time = time.time()
            c_gpu = torch.matmul(a_gpu, b_gpu)
            torch.cuda.synchronize()
            gpu_time = time.time() - start_time
            
            # CPUæµ‹è¯•
            a_cpu = a_gpu.cpu()
            b_cpu = b_gpu.cpu()
            
            start_time = time.time()
            c_cpu = torch.matmul(a_cpu, b_cpu)
            cpu_time = time.time() - start_time
            
            speedup = cpu_time / gpu_time if gpu_time > 0 else 0
            
            print(f"ğŸ“Š çŸ©é˜µå¤§å° {size}x{size}:")
            print(f"   GPUæ—¶é—´: {gpu_time:.4f} ç§’")
            print(f"   CPUæ—¶é—´: {cpu_time:.4f} ç§’")
            print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gpu_error_handling():
    """æµ‹è¯•GPUé”™è¯¯å¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•6: GPUé”™è¯¯å¤„ç†æµ‹è¯•")
    print("-" * 50)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUé”™è¯¯å¤„ç†æµ‹è¯•")
        return True
    
    try:
        device = torch.device('cuda')
        
        # æµ‹è¯•å†…å­˜ä¸è¶³é”™è¯¯
        try:
            # å°è¯•åˆ†é…è¶…å¤§å¼ é‡
            large_tensor = torch.randn(10000, 10000).to(device)
            print("âš ï¸  æ„å¤–æˆåŠŸåˆ†é…è¶…å¤§å¼ é‡")
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("âœ… æ­£ç¡®æ•è·å†…å­˜ä¸è¶³é”™è¯¯")
            else:
                print(f"âš ï¸  æ•è·åˆ°å…¶ä»–é”™è¯¯: {e}")
        
        # æµ‹è¯•è®¾å¤‡ä¸åŒ¹é…é”™è¯¯
        try:
            cpu_tensor = torch.randn(10, 10)
            gpu_tensor = torch.randn(10, 10).to(device)
            result = cpu_tensor + gpu_tensor
            print("âš ï¸  æ„å¤–æˆåŠŸæ‰§è¡Œè®¾å¤‡ä¸åŒ¹é…æ“ä½œ")
        except RuntimeError as e:
            if "device" in str(e).lower():
                print("âœ… æ­£ç¡®æ•è·è®¾å¤‡ä¸åŒ¹é…é”™è¯¯")
            else:
                print(f"âš ï¸  æ•è·åˆ°å…¶ä»–é”™è¯¯: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUé”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹GPUå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        test_gpu_availability,
        test_gpu_tensor_operations,
        test_multi_agent_gpu,
        test_gpu_memory_management,
        test_gpu_performance,
        test_gpu_error_handling
    ]
    
    passed = 0
    total = len(tests)
    start_time = time.time()
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
                print("âœ… æµ‹è¯•é€šè¿‡")
            else:
                print("âŒ æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    print(f"â±ï¸  æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f} ç§’")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰GPUæµ‹è¯•é€šè¿‡ï¼ç³»ç»ŸGPUå…¼å®¹æ€§è‰¯å¥½")
        print("ğŸš€ å¯ä»¥å®‰å…¨åœ°åœ¨GPUæœåŠ¡å™¨ä¸Šè¿è¡Œ")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†GPUæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥GPUé…ç½®")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
