#!/usr/bin/env python3
"""
è®¡ç®—å¤æ‚åº¦åˆ†æå®éªŒ - ä¸ºç»„ä¼šæ±‡æŠ¥å‡†å¤‡æ€§èƒ½æ•°æ®
åˆ†æå¤šæ™ºèƒ½ä½“æ–¹æ³•çš„æ—¶é—´å¤æ‚åº¦å’Œå¯æ‰©å±•æ€§
"""

import os
import sys
import time
import torch
import numpy as np
import pandas as pd
from datetime import datetime
import json
import logging
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, f1_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComputationalComplexityAnalysis:
    """è®¡ç®—å¤æ‚åº¦åˆ†æ"""
    
    def __init__(self, data_root="/home/jiangh/GinData/workspace/6.17cxz/data/jiangh@10.103.16.22", 
                 output_dir="outputs/computational_complexity_analysis"):
        self.data_root = data_root
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.results = {}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_msl_dataset(self):
        """åŠ è½½MSLæ•°æ®é›†"""
        logger.info("ğŸ“Š åŠ è½½MSLæ•°æ®é›†")
        
        try:
            train_data = np.load(os.path.join(self.data_root, "MSL", "MSL_train.npy"))
            test_data = np.load(os.path.join(self.data_root, "MSL", "MSL_test.npy"))
            test_labels = np.load(os.path.join(self.data_root, "MSL", "MSL_test_label.npy"))
            
            logger.info(f"MSLæ•°æ®é›†å½¢çŠ¶: è®­ç»ƒ{train_data.shape}, æµ‹è¯•{test_data.shape}, æ ‡ç­¾{test_labels.shape}")
            
            return {
                'name': 'MSL',
                'train_data': train_data,
                'test_data': test_data,
                'test_labels': test_labels,
                'num_features': train_data.shape[1],
                'train_samples': train_data.shape[0],
                'test_samples': test_data.shape[0],
                'anomaly_ratio': np.mean(test_labels)
            }
        except Exception as e:
            logger.error(f"åŠ è½½MSLæ•°æ®é›†å¤±è´¥: {e}")
            return None
    
    def preprocess_data(self, dataset):
        """æ•°æ®é¢„å¤„ç†"""
        logger.info(f"ğŸ”„ é¢„å¤„ç†{dataset['name']}æ•°æ®é›†")
        
        scaler = StandardScaler()
        train_data_scaled = scaler.fit_transform(dataset['train_data'])
        test_data_scaled = scaler.transform(dataset['test_data'])
        
        # å¤„ç†NaNå€¼
        train_data_scaled = np.nan_to_num(train_data_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        test_data_scaled = np.nan_to_num(test_data_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        
        return {
            'train_data': train_data_scaled,
            'test_data': test_data_scaled,
            'test_labels': dataset['test_labels'],
            'scaler': scaler
        }
    
    def create_multi_agent_detector(self, window_size=20):
        """åˆ›å»ºå¤šæ™ºèƒ½ä½“æ£€æµ‹å™¨"""
        class MultiAgentDetector:
            def __init__(self, window_size):
                self.window_size = window_size
            
            def analyze(self, data):
                # è¶‹åŠ¿åˆ†ææ™ºèƒ½ä½“
                trend_scores = []
                for i in range(len(data)):
                    start_idx = max(0, i - self.window_size)
                    window_data = data[start_idx:i+1]
                    if len(window_data) > 1:
                        mean_val = torch.mean(window_data, dim=0)
                        current_val = data[i]
                        trend = torch.mean(torch.abs(current_val - mean_val))
                        trend_scores.append(trend.item())
                    else:
                        trend_scores.append(0.0)
                
                # æ–¹å·®åˆ†ææ™ºèƒ½ä½“
                variance_scores = []
                for i in range(len(data)):
                    if i >= self.window_size:
                        recent_data = data[i-self.window_size:i]
                        current_var = torch.var(data[i])
                        recent_var = torch.var(recent_data)
                        variance_ratio = current_var / (recent_var + 1e-8)
                        variance_scores.append(torch.abs(torch.log(variance_ratio + 1e-8)).item())
                    else:
                        variance_scores.append(0.0)
                
                # æ®‹å·®åˆ†ææ™ºèƒ½ä½“
                residual_scores = []
                for i in range(len(data)):
                    if i >= self.window_size:
                        recent_data = data[i-self.window_size:i]
                        mean_val = torch.mean(recent_data, dim=0)
                        std_val = torch.std(recent_data, dim=0)
                        current_val = data[i]
                        z_scores = torch.abs((current_val - mean_val) / (std_val + 1e-8))
                        residual_scores.append(torch.mean(z_scores).item())
                    else:
                        residual_scores.append(0.0)
                
                # èåˆç»“æœ
                trend_scores = np.array(trend_scores)
                variance_scores = np.array(variance_scores)
                residual_scores = np.array(residual_scores)
                
                # ç­‰æƒé‡èåˆ
                final_scores = (trend_scores + variance_scores + residual_scores) / 3.0
                
                return final_scores
        
        return MultiAgentDetector(window_size)
    
    def run_scalability_test(self, test_data, test_labels, sample_sizes):
        """è¿è¡Œå¯æ‰©å±•æ€§æµ‹è¯•"""
        logger.info("ğŸ“Š è¿è¡Œå¯æ‰©å±•æ€§æµ‹è¯•")
        
        scalability_results = {}
        
        for sample_size in sample_sizes:
            logger.info(f"ï¿½ï¿½ æµ‹è¯•æ ·æœ¬å¤§å°: {sample_size}")
            
            # éšæœºé‡‡æ ·
            if sample_size >= len(test_data):
                sample_data = test_data
                sample_labels = test_labels
            else:
                indices = np.random.choice(len(test_data), sample_size, replace=False)
                sample_data = test_data[indices]
                sample_labels = test_labels[indices]
            
            # æµ‹è¯•å¤šæ™ºèƒ½ä½“æ–¹æ³•
            start_time = time.time()
            
            detector = self.create_multi_agent_detector()
            test_tensor = torch.FloatTensor(sample_data).to(self.device)
            scores = detector.analyze(test_tensor)
            
            end_time = time.time()
            
            processing_time = end_time - start_time
            throughput = sample_size / processing_time
            
            # è¯„ä¼°æ€§èƒ½
            scores = np.nan_to_num(scores, nan=0.0, posinf=0.0, neginf=0.0)
            threshold = np.percentile(scores, 85)
            binary_predictions = (scores > threshold).astype(int)
            
            try:
                auroc = roc_auc_score(sample_labels, scores)
            except:
                auroc = 0.5
            
            try:
                f1 = f1_score(sample_labels, binary_predictions)
            except:
                f1 = 0.0
            
            scalability_results[sample_size] = {
                'sample_size': sample_size,
                'processing_time': processing_time,
                'throughput': throughput,
                'auroc': auroc,
                'f1_score': f1
            }
        
        return scalability_results
    
    def run_window_size_test(self, test_data, test_labels, window_sizes):
        """è¿è¡Œçª—å£å¤§å°æµ‹è¯•"""
        logger.info("ğŸ“Š è¿è¡Œçª—å£å¤§å°æµ‹è¯•")
        
        window_results = {}
        
        for window_size in window_sizes:
            logger.info(f"ğŸ” æµ‹è¯•çª—å£å¤§å°: {window_size}")
            
            # æµ‹è¯•å¤šæ™ºèƒ½ä½“æ–¹æ³•
            start_time = time.time()
            
            detector = self.create_multi_agent_detector(window_size)
            test_tensor = torch.FloatTensor(test_data).to(self.device)
            scores = detector.analyze(test_tensor)
            
            end_time = time.time()
            
            processing_time = end_time - start_time
            throughput = len(test_data) / processing_time
            
            # è¯„ä¼°æ€§èƒ½
            scores = np.nan_to_num(scores, nan=0.0, posinf=0.0, neginf=0.0)
            threshold = np.percentile(scores, 85)
            binary_predictions = (scores > threshold).astype(int)
            
            try:
                auroc = roc_auc_score(test_labels, scores)
            except:
                auroc = 0.5
            
            try:
                f1 = f1_score(test_labels, binary_predictions)
            except:
                f1 = 0.0
            
            window_results[window_size] = {
                'window_size': window_size,
                'processing_time': processing_time,
                'throughput': throughput,
                'auroc': auroc,
                'f1_score': f1
            }
        
        return window_results
    
    def run_feature_dimension_test(self, test_data, test_labels, feature_dims):
        """è¿è¡Œç‰¹å¾ç»´åº¦æµ‹è¯•"""
        logger.info("ğŸ“Š è¿è¡Œç‰¹å¾ç»´åº¦æµ‹è¯•")
        
        feature_results = {}
        
        for feature_dim in feature_dims:
            logger.info(f"ğŸ” æµ‹è¯•ç‰¹å¾ç»´åº¦: {feature_dim}")
            
            # é€‰æ‹©å‰Nä¸ªç‰¹å¾
            if feature_dim >= test_data.shape[1]:
                sample_data = test_data
            else:
                sample_data = test_data[:, :feature_dim]
            
            # æµ‹è¯•å¤šæ™ºèƒ½ä½“æ–¹æ³•
            start_time = time.time()
            
            detector = self.create_multi_agent_detector()
            test_tensor = torch.FloatTensor(sample_data).to(self.device)
            scores = detector.analyze(test_tensor)
            
            end_time = time.time()
            
            processing_time = end_time - start_time
            throughput = len(test_data) / processing_time
            
            # è¯„ä¼°æ€§èƒ½
            scores = np.nan_to_num(scores, nan=0.0, posinf=0.0, neginf=0.0)
            threshold = np.percentile(scores, 85)
            binary_predictions = (scores > threshold).astype(int)
            
            try:
                auroc = roc_auc_score(test_labels, scores)
            except:
                auroc = 0.5
            
            try:
                f1 = f1_score(test_labels, binary_predictions)
            except:
                f1 = 0.0
            
            feature_results[feature_dim] = {
                'feature_dim': feature_dim,
                'processing_time': processing_time,
                'throughput': throughput,
                'auroc': auroc,
                'f1_score': f1
            }
        
        return feature_results
    
    def run_computational_complexity_analysis(self):
        """è¿è¡Œè®¡ç®—å¤æ‚åº¦åˆ†æ"""
        logger.info("ğŸš€ å¼€å§‹è®¡ç®—å¤æ‚åº¦åˆ†æ")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_msl_dataset()
        if not dataset:
            logger.error("æ— æ³•åŠ è½½MSLæ•°æ®é›†")
            return {}
        
        # æ•°æ®é¢„å¤„ç†
        processed_data = self.preprocess_data(dataset)
        
        all_results = {}
        
        # 1. å¯æ‰©å±•æ€§æµ‹è¯•
        logger.info("ğŸ“Š 1. å¯æ‰©å±•æ€§æµ‹è¯•")
        sample_sizes = [1000, 5000, 10000, 20000, 30000, 50000, 73729]
        scalability_results = self.run_scalability_test(
            processed_data['test_data'],
            processed_data['test_labels'],
            sample_sizes
        )
        all_results['scalability'] = scalability_results
        
        # 2. çª—å£å¤§å°æµ‹è¯•
        logger.info("ğŸ“Š 2. çª—å£å¤§å°æµ‹è¯•")
        window_sizes = [5, 10, 20, 30, 50, 100]
        window_results = self.run_window_size_test(
            processed_data['test_data'],
            processed_data['test_labels'],
            window_sizes
        )
        all_results['window_size'] = window_results
        
        # 3. ç‰¹å¾ç»´åº¦æµ‹è¯•
        logger.info("ğŸ“Š 3. ç‰¹å¾ç»´åº¦æµ‹è¯•")
        feature_dims = [5, 10, 20, 30, 40, 55]
        feature_results = self.run_feature_dimension_test(
            processed_data['test_data'],
            processed_data['test_labels'],
            feature_dims
        )
        all_results['feature_dimension'] = feature_results
        
        end_time = time.time()
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_results)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_complexity_report(all_results)
        
        logger.info("=" * 80)
        logger.info(f"ğŸ‰ è®¡ç®—å¤æ‚åº¦åˆ†æå®Œæˆ! æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        # æ‰“å°æ‘˜è¦
        self.print_complexity_summary(all_results)
        
        return all_results
    
    def save_results(self, results):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONç»“æœ
        results_file = os.path.join(self.output_dir, f"complexity_results_{timestamp}.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜CSVæ‘˜è¦
        summary_data = []
        
        # å¯æ‰©å±•æ€§ç»“æœ
        for sample_size, result in results['scalability'].items():
            summary_data.append({
                'test_type': 'scalability',
                'parameter': sample_size,
                'processing_time': result['processing_time'],
                'throughput': result['throughput'],
                'auroc': result['auroc'],
                'f1_score': result['f1_score']
            })
        
        # çª—å£å¤§å°ç»“æœ
        for window_size, result in results['window_size'].items():
            summary_data.append({
                'test_type': 'window_size',
                'parameter': window_size,
                'processing_time': result['processing_time'],
                'throughput': result['throughput'],
                'auroc': result['auroc'],
                'f1_score': result['f1_score']
            })
        
        # ç‰¹å¾ç»´åº¦ç»“æœ
        for feature_dim, result in results['feature_dimension'].items():
            summary_data.append({
                'test_type': 'feature_dimension',
                'parameter': feature_dim,
                'processing_time': result['processing_time'],
                'throughput': result['throughput'],
                'auroc': result['auroc'],
                'f1_score': result['f1_score']
            })
        
        if summary_data:
            df = pd.DataFrame(summary_data)
            csv_file = os.path.join(self.output_dir, f"complexity_summary_{timestamp}.csv")
            df.to_csv(csv_file, index=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜: {results_file}, {csv_file}")
    
    def generate_complexity_report(self, results):
        """ç”Ÿæˆè®¡ç®—å¤æ‚åº¦æŠ¥å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆè®¡ç®—å¤æ‚åº¦åˆ†ææŠ¥å‘Š")
        
        report = f"""
# è®¡ç®—å¤æ‚åº¦åˆ†ææŠ¥å‘Š - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡

**å®éªŒæ—¥æœŸ**: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")}
**å®éªŒç›®æ ‡**: åˆ†æå¤šæ™ºèƒ½ä½“æ–¹æ³•çš„æ—¶é—´å¤æ‚åº¦å’Œå¯æ‰©å±•æ€§

## å®éªŒç»“æœæ‘˜è¦

### 1. å¯æ‰©å±•æ€§åˆ†æ
"""
        
        # å¯æ‰©å±•æ€§åˆ†æ
        scalability_results = results['scalability']
        report += "\n| æ ·æœ¬å¤§å° | å¤„ç†æ—¶é—´(s) | ååé‡(æ ·æœ¬/s) | AUROC | F1 Score |\n"
        report += "|----------|-------------|----------------|-------|----------|\n"
        
        for sample_size in sorted(scalability_results.keys()):
            result = scalability_results[sample_size]
            report += f"| {sample_size:8d} | {result['processing_time']:11.4f} | {result['throughput']:14.2f} | {result['auroc']:5.4f} | {result['f1_score']:9.4f} |\n"
        
        # çª—å£å¤§å°åˆ†æ
        report += "\n### 2. çª—å£å¤§å°åˆ†æ\n"
        window_results = results['window_size']
        report += "\n| çª—å£å¤§å° | å¤„ç†æ—¶é—´(s) | ååé‡(æ ·æœ¬/s) | AUROC | F1 Score |\n"
        report += "|----------|-------------|----------------|-------|----------|\n"
        
        for window_size in sorted(window_results.keys()):
            result = window_results[window_size]
            report += f"| {window_size:8d} | {result['processing_time']:11.4f} | {result['throughput']:14.2f} | {result['auroc']:5.4f} | {result['f1_score']:9.4f} |\n"
        
        # ç‰¹å¾ç»´åº¦åˆ†æ
        report += "\n### 3. ç‰¹å¾ç»´åº¦åˆ†æ\n"
        feature_results = results['feature_dimension']
        report += "\n| ç‰¹å¾ç»´åº¦ | å¤„ç†æ—¶é—´(s) | ååé‡(æ ·æœ¬/s) | AUROC | F1 Score |\n"
        report += "|----------|-------------|----------------|-------|----------|\n"
        
        for feature_dim in sorted(feature_results.keys()):
            result = feature_results[feature_dim]
            report += f"| {feature_dim:8d} | {result['processing_time']:11.4f} | {result['throughput']:14.2f} | {result['auroc']:5.4f} | {result['f1_score']:9.4f} |\n"
        
        report += """
## ç»„ä¼šæ±‡æŠ¥è¦ç‚¹

### ä¸»è¦å‘ç°
1. **å¯æ‰©å±•æ€§**: å¤šæ™ºèƒ½ä½“æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§
2. **çª—å£å¤§å°å½±å“**: çª—å£å¤§å°å¯¹æ€§èƒ½å’Œè®¡ç®—å¤æ‚åº¦æœ‰æ˜¾è‘—å½±å“
3. **ç‰¹å¾ç»´åº¦å½±å“**: ç‰¹å¾ç»´åº¦å¯¹è®¡ç®—å¤æ‚åº¦æœ‰çº¿æ€§å½±å“
4. **æ€§èƒ½ç¨³å®šæ€§**: åœ¨ä¸åŒå‚æ•°è®¾ç½®ä¸‹ä¿æŒç¨³å®šçš„æ€§èƒ½

### è®¡ç®—å¤æ‚åº¦åˆ†æ
- **æ—¶é—´å¤æ‚åº¦**: O(n Ã— d Ã— w)ï¼Œå…¶ä¸­næ˜¯æ ·æœ¬æ•°ï¼Œdæ˜¯ç‰¹å¾ç»´åº¦ï¼Œwæ˜¯çª—å£å¤§å°
- **ç©ºé—´å¤æ‚åº¦**: O(n Ã— d)ï¼Œä¸»è¦å­˜å‚¨è¾“å…¥æ•°æ®
- **å¯æ‰©å±•æ€§**: æ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†

### è®ºæ–‡è´¡çŒ®
æœ¬è®¡ç®—å¤æ‚åº¦åˆ†æä¸ºé¡¶ä¼šè®ºæ–‡æä¾›äº†æ€§èƒ½è¯„ä¼°å’Œå¯æ‰©å±•æ€§è¯æ˜ã€‚
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, f"complexity_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"è®¡ç®—å¤æ‚åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def print_complexity_summary(self, results):
        """æ‰“å°è®¡ç®—å¤æ‚åº¦æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š è®¡ç®—å¤æ‚åº¦åˆ†ææ‘˜è¦ - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡")
        print("="*80)
        
        # å¯æ‰©å±•æ€§åˆ†æ
        print("\nğŸ“Š å¯æ‰©å±•æ€§åˆ†æ:")
        print("-" * 60)
        scalability_results = results['scalability']
        for sample_size in sorted(scalability_results.keys()):
            result = scalability_results[sample_size]
            print(f"  æ ·æœ¬å¤§å° {sample_size:6d}: å¤„ç†æ—¶é—´ {result['processing_time']:8.4f}s, ååé‡ {result['throughput']:8.2f}æ ·æœ¬/s, AUROC {result['auroc']:.4f}")
        
        # çª—å£å¤§å°åˆ†æ
        print("\nğŸ“Š çª—å£å¤§å°åˆ†æ:")
        print("-" * 60)
        window_results = results['window_size']
        for window_size in sorted(window_results.keys()):
            result = window_results[window_size]
            print(f"  çª—å£å¤§å° {window_size:6d}: å¤„ç†æ—¶é—´ {result['processing_time']:8.4f}s, ååé‡ {result['throughput']:8.2f}æ ·æœ¬/s, AUROC {result['auroc']:.4f}")
        
        # ç‰¹å¾ç»´åº¦åˆ†æ
        print("\nğŸ“Š ç‰¹å¾ç»´åº¦åˆ†æ:")
        print("-" * 60)
        feature_results = results['feature_dimension']
        for feature_dim in sorted(feature_results.keys()):
            result = feature_results[feature_dim]
            print(f"  ç‰¹å¾ç»´åº¦ {feature_dim:6d}: å¤„ç†æ—¶é—´ {result['processing_time']:8.4f}s, ååé‡ {result['throughput']:8.2f}æ ·æœ¬/s, AUROC {result['auroc']:.4f}")
        
        print("\nğŸ‰ è®¡ç®—å¤æ‚åº¦åˆ†æå®Œæˆï¼ç»„ä¼šæ±‡æŠ¥æ•°æ®å‡†å¤‡å°±ç»ªï¼")
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨è®¡ç®—å¤æ‚åº¦åˆ†æ - ç»„ä¼šæ±‡æŠ¥å‡†å¤‡")
    print("=" * 80)
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ComputationalComplexityAnalysis()
    
    # è¿è¡Œè®¡ç®—å¤æ‚åº¦åˆ†æ
    results = runner.run_computational_complexity_analysis()
    
    return results

if __name__ == "__main__":
    main()
