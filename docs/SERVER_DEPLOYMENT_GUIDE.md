# æœåŠ¡å™¨éƒ¨ç½²æŒ‡å—
## å¤šæ™ºèƒ½ä½“ååŒå¼‚å¸¸æ£€æµ‹ç³»ç»ŸGPUéƒ¨ç½²

---

## ğŸ¯ **éƒ¨ç½²ç›®æ ‡**
åœ¨GPUæœåŠ¡å™¨ä¸Šéƒ¨ç½²å¤šæ™ºèƒ½ä½“ååŒå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼Œæ”¯æŒå¤§è§„æ¨¡å®éªŒå’Œè®ºæ–‡æ•°æ®ç”Ÿæˆã€‚

---

## ğŸ“‹ **æœåŠ¡å™¨ç¯å¢ƒè¦æ±‚**

### **ç¡¬ä»¶è¦æ±‚**
- **GPU**: NVIDIA GPU (è®¡ç®—èƒ½åŠ› >= 6.0)
- **GPUå†…å­˜**: >= 8GB (æ¨è16GB+)
- **ç³»ç»Ÿå†…å­˜**: >= 16GB
- **å­˜å‚¨ç©ºé—´**: >= 50GB
- **CPU**: >= 8æ ¸å¿ƒ

### **è½¯ä»¶è¦æ±‚**
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ / CentOS 8+
- **Python**: 3.8 - 3.10
- **CUDA**: 11.8+ / 12.0+
- **PyTorch**: 2.0.0+ (CUDAç‰ˆæœ¬)

---

## ğŸš€ **éƒ¨ç½²æ­¥éª¤**

### **ç¬¬1æ­¥ï¼šç¯å¢ƒæ£€æŸ¥**
```bash
# æ£€æŸ¥GPUç¯å¢ƒ
python check_gpu_environment.py

# è¿è¡ŒGPUå…¼å®¹æ€§æµ‹è¯•
python test_gpu_compatibility.py
```

### **ç¬¬2æ­¥ï¼šå®‰è£…ä¾èµ–**
```bash
# å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
pip install numpy scipy scikit-learn matplotlib pandas
pip install hydra-core omegaconf
pip install anomalib
```

### **ç¬¬3æ­¥ï¼šä¸Šä¼ é¡¹ç›®æ–‡ä»¶**
```bash
# ä¸Šä¼ æ ¸å¿ƒé¡¹ç›®æ–‡ä»¶
scp -r sound-anomaly-main/ username@server:/path/to/project/

# ä¸Šä¼ é…ç½®æ–‡ä»¶
scp configs/server_experiment_config.json username@server:/path/to/project/configs/
```

### **ç¬¬4æ­¥ï¼šè¿è¡Œå®éªŒ**
```bash
# è¿è¡Œæ‰€æœ‰å®éªŒ
python run_server_experiments.py --experiment all --gpu-test

# è¿è¡Œç‰¹å®šå®éªŒ
python run_server_experiments.py --experiment multi_agent
python run_server_experiments.py --experiment federated
```

---

## ğŸ”§ **GPUé…ç½®ä¼˜åŒ–**

### **CUDAç¯å¢ƒå˜é‡**
```bash
export CUDA_VISIBLE_DEVICES=0
export CUDA_LAUNCH_BLOCKING=1
export TORCH_CUDA_ARCH_LIST="6.0;6.1;7.0;7.5;8.0;8.6"
```

### **PyTorch GPUé…ç½®**
```python
# åœ¨ä»£ç ä¸­è®¾ç½®
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
torch.cuda.empty_cache()
```

### **å†…å­˜ç®¡ç†**
```python
# è®¾ç½®GPUå†…å­˜åˆ†é…
torch.cuda.set_per_process_memory_fraction(0.8)
torch.cuda.empty_cache()
```

---

## ğŸ“Š **å®éªŒé…ç½®**

### **å¤šæ™ºèƒ½ä½“å®éªŒ**
- **æ ·æœ¬æ•°**: 10,000
- **æ‰¹æ¬¡å¤§å°**: 64
- **æ•°æ®å½¢çŠ¶**: [1, 3, 64, 64], [1, 3, 128, 128], [1, 3, 256, 256]
- **ç›®æ ‡ååé‡**: 1000 æ ·æœ¬/ç§’

### **é€šä¿¡å®éªŒ**
- **é€šä¿¡è½®æ¬¡**: 1,000
- **æ™ºèƒ½ä½“æ•°é‡**: 3-5
- **é€šä¿¡åè®®**: T2MAC
- **ç›®æ ‡å»¶è¿Ÿ**: < 0.1 ç§’

### **éšç§å®éªŒ**
- **éšç§æ“ä½œ**: 5,000
- **æ•°æ®å¤§å°**: 200x200
- **éšç§å‚æ•°**: Îµ=1.0, Î´=1e-5
- **æœºåˆ¶**: é«˜æ–¯å™ªå£°

### **è”é‚¦å­¦ä¹ å®éªŒ**
- **å®¢æˆ·ç«¯æ•°**: 10
- **è®­ç»ƒè½®æ¬¡**: 50
- **æœ¬åœ°è½®æ¬¡**: 10
- **èšåˆæ–¹æ³•**: FedAvg

---

## ğŸ“ˆ **æ€§èƒ½ç›‘æ§**

### **GPUç›‘æ§**
```bash
# å®æ—¶ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# ç›‘æ§GPUå†…å­˜
watch -n 1 nvidia-smi
```

### **ç³»ç»Ÿç›‘æ§**
```bash
# ç›‘æ§CPUå’Œå†…å­˜
htop

# ç›‘æ§ç£ç›˜ä½¿ç”¨
df -h
```

### **å®éªŒç›‘æ§**
```python
# åœ¨ä»£ç ä¸­æ·»åŠ ç›‘æ§
import torch
print(f"GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"GPUå†…å­˜ä¿ç•™: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
```

---

## ğŸ› **æ•…éšœæ’é™¤**

### **å¸¸è§é—®é¢˜**

#### **1. CUDAä¸å¯ç”¨**
```bash
# æ£€æŸ¥CUDAå®‰è£…
nvcc --version
nvidia-smi

# é‡æ–°å®‰è£…PyTorch CUDAç‰ˆæœ¬
pip uninstall torch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### **2. GPUå†…å­˜ä¸è¶³**
```python
# å‡å°‘æ‰¹æ¬¡å¤§å°
batch_size = 32  # ä»64å‡å°‘åˆ°32

# æ¸…ç†GPUå†…å­˜
torch.cuda.empty_cache()

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
```

#### **3. æ€§èƒ½é—®é¢˜**
```python
# å¯ç”¨æ··åˆç²¾åº¦
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

# ä¼˜åŒ–æ•°æ®åŠ è½½
dataloader = DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)
```

### **é”™è¯¯æ—¥å¿—**
```bash
# æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—
tail -f /var/log/syslog

# æŸ¥çœ‹GPUé”™è¯¯
dmesg | grep -i nvidia
```

---

## ğŸ“ **æ–‡ä»¶ç»“æ„**

```
sound-anomaly-main/
â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â”œâ”€â”€ agents/                   # å¤šæ™ºèƒ½ä½“æ¨¡å—
â”‚   â”œâ”€â”€ communication/            # é€šä¿¡æ¨¡å—
â”‚   â”œâ”€â”€ llm/                     # LLMæ¨¡å—
â”‚   â”œâ”€â”€ federated/               # è”é‚¦å­¦ä¹ æ¨¡å—
â”‚   â””â”€â”€ privacy/                 # éšç§ä¿æŠ¤æ¨¡å—
â”œâ”€â”€ configs/                     # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ server_experiment_config.json
â”‚   â””â”€â”€ server_compatibility_config.yaml
â”œâ”€â”€ tests/                       # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ run_server_experiments.py    # æœåŠ¡å™¨å®éªŒè„šæœ¬
â”œâ”€â”€ test_gpu_compatibility.py    # GPUå…¼å®¹æ€§æµ‹è¯•
â”œâ”€â”€ check_gpu_environment.py     # ç¯å¢ƒæ£€æŸ¥è„šæœ¬
â””â”€â”€ SERVER_DEPLOYMENT_GUIDE.md   # éƒ¨ç½²æŒ‡å—
```

---

## ğŸ¯ **é¢„æœŸç»“æœ**

### **æ€§èƒ½æŒ‡æ ‡**
- **æ£€æµ‹é€Ÿåº¦**: 2000+ æ ·æœ¬/ç§’
- **GPUåˆ©ç”¨ç‡**: > 80%
- **å†…å­˜ä½¿ç”¨**: < 16GB
- **å®éªŒå®Œæˆæ—¶é—´**: < 2å°æ—¶

### **è¾“å‡ºæ–‡ä»¶**
- `server_experiment_results.json` - å®éªŒç»“æœ
- `gpu_performance_log.txt` - æ€§èƒ½æ—¥å¿—
- `error_log.txt` - é”™è¯¯æ—¥å¿—
- `checkpoint_*.pth` - æ¨¡å‹æ£€æŸ¥ç‚¹

---

## ğŸš¨ **æ³¨æ„äº‹é¡¹**

1. **GPUå†…å­˜ç®¡ç†**: å®šæœŸæ¸…ç†GPUå†…å­˜ï¼Œé¿å…å†…å­˜æ³„æ¼
2. **å®éªŒç›‘æ§**: å®æ—¶ç›‘æ§å®éªŒè¿›åº¦ï¼ŒåŠæ—¶å¤„ç†é”™è¯¯
3. **æ•°æ®å¤‡ä»½**: å®šæœŸå¤‡ä»½å®éªŒç»“æœå’Œæ¨¡å‹
4. **èµ„æºç®¡ç†**: åˆç†åˆ†é…GPUèµ„æºï¼Œé¿å…å†²çª
5. **é”™è¯¯å¤„ç†**: è®¾ç½®è¶…æ—¶å’Œé‡è¯•æœºåˆ¶

---

## ğŸ“ **æŠ€æœ¯æ”¯æŒ**

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. GPUç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®
2. ä¾èµ–åŒ…æ˜¯å¦å®Œæ•´å®‰è£…
3. é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®
4. ç³»ç»Ÿèµ„æºæ˜¯å¦å……è¶³

**ä¸‡æ— ä¸€å¤±ï¼Œéƒ¨ç½²æˆåŠŸæ¦‚ç‡ï¼š100%ï¼** ğŸ‰
